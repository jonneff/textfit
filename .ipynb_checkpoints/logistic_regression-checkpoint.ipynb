{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "# Force matplotlib to not use any Xwindows backend.\n",
    "matplotlib.use('Agg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    " \n",
    "# Path for spark source folder\n",
    "os.environ['SPARK_HOME'] = \"/usr/local/spark\"\n",
    "\n",
    "# Append pyspark to Python Path\n",
    "sys.path.append(\"/usr/local/spark/python\")\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "# Load in the testing code and check to see if your answer is correct\n",
    "# If incorrect it will report back '1 test failed' for each failed test\n",
    "# Make sure to rerun any cell you change before trying the test again\n",
    "from test_helper import Test\n",
    "from pyspark.sql import SQLContext\n",
    "# sc = SparkContext() # not needed in IPython notebook.\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "# Append afinn to Python Path and import afinn.  Used for pulling data from percentiles.\n",
    "sys.path.append(\"/usr/local/lib/python2.7/dist-packages/afinn\")\n",
    "from afinn import Afinn\n",
    "\n",
    "# Stuff for logistic regression\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "from pyspark.sql.types import StructField, BooleanType, StringType, LongType, StructType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sys.path.append(\"/usr/local/lib/python2.7/dist-packages\")\n",
    "from tdigest import TDigest\n",
    "from numpy.random import random\n",
    "from operator import add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.0 Read data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in data.  For now, one file on S3.  Reading one 85 MB file on S3 doesn't seem to take too long. \n",
    "\n",
    "<B>ADD CODE TO READ multiple files FROM S3.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "df = sqlContext.read.json(\"/home/ubuntu/RC_2007-10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "print datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ts1 = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ts2 = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print ts2-ts1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "df = sqlContext.read.json(\"s3n://reddit-comments/2007/RC_2007-10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>IMMEDIATELY FILTER DOWN TO [note i checked and this is correct spelling]:\n",
    "\n",
    "\n",
    "leagueoflegends\n",
    "\n",
    "GirlGamers\n",
    "\n",
    "pics\n",
    "\n",
    "politics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(archived=True, author=u'BraveSirRobin', author_flair_css_class=None, body=u'Some of the linux distros, as well as BSD, make this really easy. You don\\'t need to tweak anything, it\\'s \"ready to compile\". I don\\'t bother with it myself.', controversiality=0, created_utc=u'1193875218', distinguished=None, downs=0, edited=u'false', gilded=0, id=u'c02chew', link_id=u't3_5zjl1', name=u't1_c02chew', parent_id=u't1_c02ch4f', retrieved_on=1427424835, score=2, score_hidden=False, subreddit=u'reddit.com', subreddit_id=u't5_6', ups=2),\n",
       " Row(archived=True, author=u'kobes', author_flair_css_class=None, body=u\"Don't you think there's a line (sometimes blurry) between free speech and harassment?  Would it be ok for me to express my opinions through a megaphone outside your window at 3 in the morning?\", controversiality=0, created_utc=u'1193875226', distinguished=None, downs=0, edited=u'false', gilded=0, id=u'c02chex', link_id=u't3_5zk4n', name=u't1_c02chex', parent_id=u't1_c02ch63', retrieved_on=1427424835, score=1, score_hidden=False, subreddit=u'politics', subreddit_id=u't5_2cneq', ups=1),\n",
       " Row(archived=True, author=u'Imperator', author_flair_css_class=None, body=u'Vote Pedro\\r\\n', controversiality=0, created_utc=u'1193875244', distinguished=None, downs=0, edited=u'false', gilded=0, id=u'c02chey', link_id=u't3_5zj73', name=u't1_c02chey', parent_id=u't3_5zj73', retrieved_on=1427424835, score=-6, score_hidden=False, subreddit=u'reddit.com', subreddit_id=u't5_6', ups=-6),\n",
       " Row(archived=True, author=u'sketerpot', author_flair_css_class=None, body=u\"If you open a standard 600CCA car battery, you'll find six lead-acid cells in series.\", controversiality=0, created_utc=u'1193875247', distinguished=None, downs=0, edited=u'false', gilded=0, id=u'c02chez', link_id=u't3_5zgd4', name=u't1_c02chez', parent_id=u't1_c02c8c4', retrieved_on=1427424835, score=1, score_hidden=False, subreddit=u'reddit.com', subreddit_id=u't5_6', ups=1),\n",
       " Row(archived=True, author=u'[deleted]', author_flair_css_class=None, body=u'[deleted]', controversiality=0, created_utc=u'1193875253', distinguished=None, downs=0, edited=u'false', gilded=0, id=u'c02chf0', link_id=u't3_5zimk', name=u't1_c02chf0', parent_id=u't3_5zimk', retrieved_on=1427424835, score=1, score_hidden=False, subreddit=u'politics', subreddit_id=u't5_2cneq', ups=1)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define json schema to speed up reading json files in S3\n",
    "\n",
    "fields = [StructField(\"archived\", BooleanType(), True),\n",
    "        StructField(\"author\", StringType(), True),\n",
    "        StructField(\"author_flair_css_class\", StringType(), True),\n",
    "        StructField(\"body\", StringType(), True),\n",
    "        StructField(\"controversiality\", LongType(), True),\n",
    "        StructField(\"created_utc\", StringType(), True),\n",
    "        StructField(\"distinguished\", StringType(), True),\n",
    "        StructField(\"downs\", LongType(), True),\n",
    "        StructField(\"edited\", StringType(), True),\n",
    "        StructField(\"gilded\", LongType(), True),\n",
    "        StructField(\"id\", StringType(), True),\n",
    "        StructField(\"link_id\", StringType(), True),\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"parent_id\", StringType(), True),\n",
    "        StructField(\"retrieved_on\", LongType(), True),\n",
    "        StructField(\"score\", LongType(), True),\n",
    "        StructField(\"score_hidden\", BooleanType(), True),\n",
    "        StructField(\"subreddit\", StringType(), True),\n",
    "        StructField(\"subreddit_id\", StringType(), True),\n",
    "        StructField(\"ups\", LongType(), True)]\n",
    "\n",
    "df = sqlContext.read.json(\"s3n://reddit-comments/2007\", StructType(fields))\n",
    "df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(archived=True, author=u'[deleted]', author_flair_css_class=None, body=u\"It's not the quality of medicine that I complain about, it's the availability of medicine.  If I were dying of cancer, I would like to make the choice myself to decide how much risk I was willing to accept in my treatment.  \\r\\n\\r\\nAnd the cleanliness of hospitals is laughable.  This article estimates that 94,360 drug resistant staph infections occurred in hospitals in 2005.  \\r\\n\\r\\nhttp://www.sciencedaily.com/releases/2007/10/071016160644.htm\\r\\n\\r\\n103,000 deaths in 2000, according to this article.  75% of which should have been preventable.\\r\\n\\r\\nhttp://www.cbsnews.com/stories/2002/07/20/health/main515755.shtml\\r\\n\\r\\nHow's that regulation working out?\", controversiality=0, created_utc=u'1195445060', distinguished=None, downs=0, edited=u'false', gilded=0, id=u'c02hqgj', link_id=u't3_60vxo', name=u't1_c02hqgj', parent_id=u't1_c02hoj4', retrieved_on=1427422385, score=0, score_hidden=False, subreddit=u'politics', subreddit_id=u't5_2cneq', ups=0),\n",
       " Row(archived=True, author=u'facewarts', author_flair_css_class=None, body=u'Dear liberal schmucks.  This is not the first time nor the last time.\\n\\nJust ignore, be a good liberal and just drink the kool aid !', controversiality=0, created_utc=u'1195445108', distinguished=None, downs=0, edited=u'false', gilded=0, id=u'c02hqgr', link_id=u't3_60vhf', name=u't1_c02hqgr', parent_id=u't3_60vhf', retrieved_on=1427422384, score=-1, score_hidden=False, subreddit=u'politics', subreddit_id=u't5_2cneq', ups=-1),\n",
       " Row(archived=True, author=u'redfishvanish', author_flair_css_class=None, body=u\"Is it just me, or does Chuck look like he's been under one-too-many tanning lamps?\", controversiality=0, created_utc=u'1195445207', distinguished=None, downs=0, edited=u'true', gilded=0, id=u'c02hqgy', link_id=u't3_60wfv', name=u't1_c02hqgy', parent_id=u't3_60wfv', retrieved_on=1427422384, score=3, score_hidden=False, subreddit=u'politics', subreddit_id=u't5_2cneq', ups=3),\n",
       " Row(archived=True, author=u'[deleted]', author_flair_css_class=None, body=u'[deleted]', controversiality=0, created_utc=u'1195445219', distinguished=None, downs=0, edited=u'false', gilded=0, id=u'c02hqh0', link_id=u't3_60uez', name=u't1_c02hqh0', parent_id=u't1_c02hlfd', retrieved_on=1427422384, score=1, score_hidden=False, subreddit=u'politics', subreddit_id=u't5_2cneq', ups=1),\n",
       " Row(archived=True, author=u'gbacon', author_flair_css_class=None, body=u\"The time of each physician is scarce. Therefore, the time of the system that you describe, *i.e.*, the time of all physicians in aggregate, is also scarce.\\n\\nWe live in a world of scarcity. There's no getting around this basic principle.\", controversiality=0, created_utc=u'1195445234', distinguished=None, downs=0, edited=u'false', gilded=0, id=u'c02hqh1', link_id=u't3_60vxo', name=u't1_c02hqh1', parent_id=u't1_c02hqfg', retrieved_on=1427422384, score=3, score_hidden=False, subreddit=u'politics', subreddit_id=u't5_2cneq', ups=3)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter down to subreddits of interest\n",
    "\n",
    "df2 = (df.filter(  (df.subreddit == u'leagueoflegends') \n",
    "                 | (df.subreddit == u'GirlGamers')\n",
    "                 | (df.subreddit == u'pics')\n",
    "                 | (df.subreddit == u'politics') )                   \n",
    "           .persist(StorageLevel.MEMORY_AND_DISK_SER)\n",
    "           )\n",
    "# df2.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'politics': True, u'pics': False, u'leagueoflegends': False, u'GirlGamers': False}\n"
     ]
    }
   ],
   "source": [
    "# Are there ANY records for the list of subreddits?\n",
    "subreddits = [u'leagueoflegends', u'GirlGamers', u'pics', u'politics']\n",
    "\n",
    "def is_there(df,srList):\n",
    "    return {key : True if df2.filter((df.subreddit == key)).take(1) else False for key in srList }\n",
    "\n",
    "isInData = is_there(df2, subreddits)\n",
    "print isInData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find out how many records there are for each subreddit\n",
    "subreddits = [u'leagueoflegends', u'GirlGamers', u'pics', u'politics']\n",
    "\n",
    "def create_counts(df,srList):\n",
    "    return {key : df2.filter((df.subreddit == key)).count() for key in srList}\n",
    "\n",
    "srCounts = create_counts(df2, subreddits)\n",
    "print srCounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'c0299ce',\n",
       "  (u'Mass torture at Guantanamo?  I would love to see a source on that one.  The only I have heard there is sleep deprivation and loud music.\\n\\nAbu Gharib?  That was an isolated incident and only the \"bush is hitler\" nutjobs think the administration had a direct hand in it.\\n\\nRenditioning?  I hate to break it to you but enemy forces fighting on a field of battle *out of uniform* have no rights under Geneva. None, zip, nada.  Call it wrong all you like, but illegal its not.\\n\\nFISA?   This has been done in drug enforcement long before WOT.  And there is no spying on \\'millions\\' of citizens either.\\n\\nThe roving wiretaps are on *individuals* which we are trying to monitor, therefore law enforcement should be able to track them on new phones. If you think terrorists under surveillance should be able to just grab a new cell phone at Wal-mart and start the whole judicial review process all over again you are crazy. \\n',\n",
       "   1192451036,\n",
       "   u't3_2zrn9',\n",
       "   u't1_c2zxpw',\n",
       "   0,\n",
       "   u'politics',\n",
       "   u't5_2cneq')),\n",
       " (u'c0299fh',\n",
       "  (u'&gt; The White House stuck to polite, if restrained, congratulations. \"Obviously, it\\'s an important recognition, and we\\'re sure the vice president is thrilled,\" spokesman Tony Fratto told reporters aboard Air Force One heading here Friday.\\n\\nWhat? Dick Cheney is happy for Al Gore? And what\\'s Dick Cheney doing on Air Force One? Has he taken over for real now and isn\\'t just lurking behind White House bushes?\\n\\nOh.. oh... yeh, now I get it. The American habit of continuing to address an ex-office-holder by their former title. Once a veep always a veep. Sort of like Colonel Sanders.',\n",
       "   1192452090,\n",
       "   u't3_2zswb',\n",
       "   u't3_2zswb',\n",
       "   0,\n",
       "   u'politics',\n",
       "   u't5_2cneq')),\n",
       " (u'c0299gb',\n",
       "  (u'You *must* worship at the GORACLE:\\n\\nhttp://i65.photobucket.com/albums/h219/pjcomix/blog/goracle.jpg\\n',\n",
       "   1192452303,\n",
       "   u't3_2zswb',\n",
       "   u't1_c2zxkz',\n",
       "   0,\n",
       "   u'politics',\n",
       "   u't5_2cneq')),\n",
       " (u'c0299ge',\n",
       "  (u\"I'm far from self-loathing, and I'm about as sane as it gets.  The people who don't understand how our (The U.S.'s) policies in the middle east would push someone to commit an act of terror is mentally diseased.\\r\\n\\r\\nDo the people deserve to die?  No.  Can I understand why they did?  Yes.\",\n",
       "   1192452356,\n",
       "   u't3_2ybqz',\n",
       "   u't1_c2yh6g',\n",
       "   0,\n",
       "   u'politics',\n",
       "   u't5_2cneq')),\n",
       " (u'c0299gh',\n",
       "  (u\" Yes, he seemed to squeeze too much about the financial system into a short space of time, so making a little too generalized.\\n\\nThere is more on how money is newly issued and to who it is distributed first, in 3 good documentaries...\\n\\n'Money As Debt':\\n\\nhttp://video.google.com/videoplay?docid=-9050474362583451279 (45 mins)\\n\\n'Fiat Empire':\\n\\nhttp://video.google.com/videoplay?docid=5232639329002339531 (1 hour)\\n\\n'The Money Masters':\\n\\npart 1. http://video.google.com/videoplay?docid=6076118677860424204 (1 hour 45 mins)\\n\\npart 2. http://video.google.com/videoplay?docid=-7336845760512239683 (1 hour 45 mins)\\n\\nI'm not for a Gold backed currency.  I'm for a system of free market competitive demand-led banking (no cartel centralization, monopoly or instability) like what we had for most of the time during the first half of the United States history before the Rothschild powerful banks with the help of Warburg and Rockefeller took control and started issuing Fiat Money as debt and forcing everyone to use their currency with the help of law and executive orders, thus talking away from the people instead of investing in the future of our children.\\n\\n \",\n",
       "   1192452403,\n",
       "   u't3_2ztbv',\n",
       "   u't1_c2zxjo',\n",
       "   3,\n",
       "   u'politics',\n",
       "   u't5_2cneq'))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Put Dataframe into vanilla RDD\n",
    "\n",
    "rRDD = (df2.map(lambda r: (r.id, (r.body, int(r.created_utc), r.link_id, r.parent_id, int(r.score), r.subreddit, r.subreddit_id)))\n",
    "          .persist(StorageLevel.MEMORY_AND_DISK_SER)\n",
    "       )\n",
    "rRDD.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Filter to include only extreme up and down votes (top 3% of subreddit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter to retain only records that are top or bottom 3% in comment score (upvotes-downvotes) of their subreddit.  Reduces dataset for all subsequent processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'politics', [0]),\n",
       " (u'politics', [0]),\n",
       " (u'politics', [0]),\n",
       " (u'politics', [0]),\n",
       " (u'politics', [3])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get into (subreddit, score) format.  Need to persist or Spark has to recreate RDD for all Digest processing each time.\n",
    "srScoreRDD = rRDD.map(lambda (k, v):  (v[5], [v[4]])).persist(StorageLevel.MEMORY_AND_DISK_SER) \n",
    "srScoreRDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Helper function creates a t-digest sparse representation of probability distribution from set of values\n",
    "def digest_batch(values):\n",
    "    digest = TDigest()\n",
    "    digest.batch_update(values)\n",
    "    return [digest]\n",
    "\n",
    "# Create separate Python dicts with t-digests for each subreddit of interest.\n",
    "\n",
    "def createDigestDictionary(RDD, srList, isIn):\n",
    "    for key in srList:\n",
    "        \n",
    "    \n",
    "\n",
    "# WHAT IF THERE ARE NO 'leagueoflegends' items in RDD?\n",
    "llDigest = {}\n",
    "if isInData[u'leagueoflegends']:\n",
    "    llDigest = (srScoreRDD.filter(lambda (k,v): k == u'leagueoflegends')\n",
    "                       # .map(lambda (k,v): v)\n",
    "                       # .mapPartitions(digest_batch)\n",
    "                       # .treeReduce(add)\n",
    "             ).take(5) #.collectAsMap()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ggDigest = (srScoreRDD.filter(lambda (k,v): k == 'GirlGamers')\n",
    "                       .map(lambda (k,v): v)\n",
    "                       .mapPartitions(digest_batch)\n",
    "                       .treeReduce(add)\n",
    "             ).take() # .collectAsMap()\n",
    "\n",
    "piDigest = (srScoreRDD.filter(lambda (k,v): k == 'pics')\n",
    "                       .map(lambda (k,v): v)\n",
    "                       .mapPartitions(digest_batch)\n",
    "                       .treeReduce(add)\n",
    "             ).take() # .collectAsMap()\n",
    "\n",
    "poDigest = (srScoreRDD.filter(lambda (k,v): k == 'politics')\n",
    "                       .map(lambda (k,v): v)\n",
    "                       .mapPartitions(digest_batch)\n",
    "                       .treeReduce(add)\n",
    "             ).take() # .collectAsMap()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Combine dictionaries into a single dictionary\n",
    "\"\"\"\n",
    "subredditDigest = llDigest.update(ggDigest).update(piDigest).update(poDigest)\n",
    "\n",
    "# and back to our regularly scheduled programming\n",
    "\n",
    "lowPercentile, highPercentile = 3, 97\n",
    "\n",
    "srDigestR = {key : (round(subredditDigest[key].percentile(lowPercentile)), \n",
    "                    round(subredditDigest[key].percentile(highPercentile))) \n",
    "             for key in subredditDigest.keys()}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "ts1 = time.time()\n",
    "dfOld = sqlContext.read.json(\"s3n://reddit-comments/2007/\")\n",
    "ts2 = time.time()\n",
    "print ts2 - ts1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "df2.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Find minimum comment timestamp for each post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each post link_id, find minimum created_utc timestamp [can't trust data is ordered by time] and store in key-value pair (pair RDD) {link_id: min_created_utc},  (Plan B:  set up API to get timestamp for all posts in Reddit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort to find minimum timestamp created_utc for each post link_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "minTimeRDD = (rRDD.map(lambda (k, v): (v[2],v[1])) # maps to (link_id, created_utc)\n",
    "                  .reduceByKey(lambda a, b:  a if a < b else b)\n",
    "              ).persist(StorageLevel.MEMORY_AND_DISK_SER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find 3% and 97% percentiles for each subreddit.  Use T-digest data structure for highly accurate approximate percentiles: \n",
    "http://dataorigami.net/blogs/napkin-folding/19055451-percentile-and-quantile-estimation-of-big-data-the-t-digest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def digest1(value):\n",
    "    digest = TDigest()\n",
    "    digest.update(value)\n",
    "    return digest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def digest_batch(values):\n",
    "    digest = TDigest()\n",
    "    digest.batch_update(values)\n",
    "    return [digest]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "testd = digest_batch([1,1,4,5,6,7,5,4,3,2,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to get at scores for each subreddit, THEN map using digest.\n",
    "\n",
    "tigest WAS a bottleneck, taking ~20 minutes for 85 MB dataset when ingesting line by line.  I refactored to use reduceByKey() to create a big list, then feed the list into tdigest in batch, which goes about 20x faster.  \n",
    "\n",
    "<b>NEED TO SPLIT INTO TRAINING, VAL AND TEST BEFORE CREATING SUBREDDIT DIGEST ON TRAINING DATA ONLY.  OTHERWISE I'M CHEATING.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "subredditDigestRDD = (rRDD.map(lambda (k, v):  (v[5], v[4]))\n",
    "                        .map(lambda (k, v): (k, digest1(v)))  \n",
    "                        .reduceByKey(lambda a, b:  a + b)\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "subredditDigestRDD = (rRDD.map(lambda (k, v):  (v[5], [v[4]]))\n",
    "                          .reduceByKey(lambda a, b:  a + b)\n",
    "                          .map(lambda (k, v): (k, digest_batch(v)))  \n",
    "                        \n",
    "                   ).persist(StorageLevel.MEMORY_AND_DISK_SER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "type(subredditDigestRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REFACTOR options to increase speed.  I went with 2.  \n",
    "1.  RUN TDIGEST ON PARTITION INSTEAD OF EACH VALUE WHEN CREATING subredditDigestRDD.\n",
    "2.  reduceByKey() on subreddits and create huge lists for each subreddit, then create tdigests from huge lists.\n",
    "3.  reduceByKey() on year-month and create huger lists for each month, then create tdigests from huger lists.\n",
    "4.  Some combination of 2 and 3.\n",
    "\n",
    "NOTE:  Ronak says that reduceByKey() won't generate a list but groupByKey() will. He was wrong- I can add lists. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "ts1 = time.time()\n",
    "subredditDigest = subredditDigestRDD.collectAsMap()\n",
    "ts2 = time.time()\n",
    "print ts2-ts1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "print ts2-ts1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "type(subredditDigest['politics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "subredditDigest['politics'].percentile(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "subredditDigest['politics'].percentile(97)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alyssa's values for politics were (-6.0, 24.0) rounded.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set limits for what data to include by percentile.  Use only data less than lowPercentile or greater than highPercentile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "lowPercentile, highPercentile = 3, 97\n",
    "srDigestR = {key : (round(subredditDigest[key].percentile(lowPercentile)), \n",
    "                    round(subredditDigest[key].percentile(highPercentile))) \n",
    "             for key in subredditDigest.keys()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "srDigestR['politics']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "len(srDigestR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "print srDigestR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "print len(srDigestR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "print df.filter(df['subreddit'] == 'politics').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "for key in srDigestR.keys():\n",
    "    print key, df.filter(df['subreddit'] == key).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter records to retain only top and bottom 3% of comment scores.\n",
    "\n",
    "<b>NOTE:  TRY BROADCAST srDigestR WHEN I MOVE TO CLUSTER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rRDDExtreme = rRDD.filter(lambda (k,v): v[4] < srDigestR[v[5]][0] or v[4] > srDigestR[v[5]][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "rRDDExtreme.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0  Calculate timeSince"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate time since post was created based on created_utc and min_created_utc from pair RDD.  In Alyssa's IPython notebook this is called timeSince.  In her R code it's called recency.  \n",
    "\n",
    "For this I need a left outer join.  For each element (k, v) in self, the resulting RDD will either contain all pairs (k, (v, w)) for w in other, or the pair (k, (v, None)) if no elements in other have key k.  I need to know if there are records in my comment dataset for which there is no \"minimum time\", as this would indicate a processing error.\n",
    "\n",
    "Map RDD to get post link_id as key, then join with minTimeRDD.\n",
    "\n",
    "<b>CAN I USE DATAFRAMES FOR ALL THIS? GETTING TIRED OF REMEMBERING WHAT v[4] IS.  </b>\n",
    "\n",
    "The only data I need for regression are:  C(subreddit) + timeSince + commentLength + posNegDiff.  Need to keep only comment id (key), score, subreddit, timeSince, comment text from this step.\n",
    "\n",
    "Format of output RDD is (id,(body,timeSince,score,subreddit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rRDDXts = (rRDDExtreme.map(lambda (k,v):  (v[2],(k,v[0],v[1],v[2],v[3],v[4],v[5],v[6])))  # pull link_id as key\n",
    "                      .leftOuterJoin(minTimeRDD) # join on link_id (post)\n",
    "                      .map(lambda (link_id,(x,min_utc)):  (x[0], (x[1],x[2]-min_utc,x[5],x[6])))\n",
    "                      .persist(StorageLevel.MEMORY_AND_DISK_SER)\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "rRDDXts.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.0  Calculate commentLength"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean comment body and calculate commentLength.\n",
    "\n",
    "R gsub:\n",
    "gsub(pattern, replacement, x, ignore.case = FALSE, perl = FALSE, fixed = FALSE, useBytes = FALSE)\n",
    "\n",
    "Python re:\n",
    "re.sub(pattern, repl, string, count=0, flags=0).  Return the string obtained by replacing the leftmost non-overlapping occurrences of pattern in string by the replacement repl.\n",
    "\n",
    "<B>NOTE:  ALYSSA REMOVED QUOTED COMMENTS.  I REMOVING THEM ALSO BUT IN MY FIRST EXAMPLE I FOUND A \"MADE UP\" QUOTE THAT ISN'T REALLY QUOTING SOMEONE ELSE'S POST.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanup(body):\n",
    "\n",
    "\t# Recode HTML codes\n",
    "\tbody = re.sub(\"&gt;\", \">\", body)\n",
    "\tbody = re.sub(\"&lt;\", \"<\", body)\n",
    "\tbody = re.sub(\"&amp;\", \"&\", body)\n",
    "\tbody = re.sub(\"&nbsp;\", \" \", body)\n",
    "\n",
    "\t# Remove deleted\n",
    "\tbody = re.sub(\"^[deleted]$\", \"\", body)\n",
    "\n",
    "\t# Remove URL\n",
    "\tbody = re.sub(\"http[[:alnum:][:punct:]]*\", \" \", body) # url\n",
    "\n",
    "\t# Remove /r/subreddit, /u/user\n",
    "\tbody = re.sub(\"/r/[[:alnum:]]+|/u/[[:alnum:]]+\", \" \", body)\n",
    "\n",
    "\t# Remove quoted comments\n",
    "\tbody = re.sub(\"(>.*?\\\\n\\\\n)+\", \" \", body)\n",
    "\n",
    "\t# Remove control characters (\\n, \\b)\n",
    "\tbody = re.sub(\"[[:cntrl:]]\", \" \", body)\n",
    "\n",
    "\t# Remove single quotation marks (contractions)\n",
    "\tbody = re.sub(\"'\", \"\", body)\n",
    "\n",
    "\t# Remove punctuation\n",
    "\tbody = re.sub(\"[[:punct:]]\", \" \", body)\n",
    "\n",
    "\t# Replace multiple spaces with single space\n",
    "\tbody = re.sub(\"\\\\s+\", \" \", body) # Multiple spaces\n",
    "\t# body = re.sub(\"^\\\\s+\", \"\", body) # Space at the start of the string\n",
    "\t# body = re.sub(\"+\\\\s$\", \"\", body) # Space at the end of the string\n",
    "\tbody = body.strip()\n",
    "\n",
    "\t# Lower case\n",
    "\tbody = body.lower()\n",
    "\n",
    "\t# Return comment length (number of words) and body (cleaned up text)\n",
    "\treturn body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "clbody = cleanup(u\"Basically, the hospital's position amounts to:\\n\\n&gt; If she can't hold her roofies she deserves to be a**f****d and denied medical care and collection of evidence!\\n\\nNot the *most* progressive attitude...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "print len(clbody.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "print clbody"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current format of RDD:  (id,(body,timeSince,score,subreddit))\n",
    "Format of rRDDXtscl:  (id,(commentLength,body,timeSince,score,subreddit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rRDDXtscl = (rRDDXts.map(lambda (id,(body,timeSince,score,subreddit)): (id,(cleanup(body),timeSince,score,subreddit)))\n",
    "                    .map(lambda (id,(body,timeSince,score,subreddit)): (id,(len(body.split()),body,timeSince,score,subreddit)))\n",
    "             ).persist(StorageLevel.MEMORY_AND_DISK_SER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.0 (Filter out exclusions if necessary; skip for now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out exclusions.  Further reduces dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.0 Run sentiment analysis and calculate posNegDiff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use AFINN model to do sentiment analysis.\n",
    "\n",
    "Finn Årup Nielsen, \"A new ANEW: evaluation of a word list for sentiment analysis in microblogs\" , Proceedings of the ESWC2011 Workshop on 'Making Sense of Microposts': Big things come in small packages 718 in CEUR Workshop Proceedings: 93-98. 2011 May. Matthew Rowe, Milan Stankovic, Aba-Sah Dadzie, Mariann Hardey (editors)\n",
    "\n",
    "<B>I'M WONDERING IF CREATING AN AFINN OBJECT EACH TIME IS TAKING TOO LONG.  MIGHT WANT TO REFACTOR THIS TO A PYTHON FUNCTION WITH A DICT LOOKUP AND USE A BROADCAST VARIABLE.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentiment(body):\n",
    "    afinn = Afinn()\n",
    "    return afinn.score(body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "sentiment(\"This is utterly excellent!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rRDDtscls = (rRDDXtscl.map(lambda (id,(commentLength,body,timeSince,score,subreddit)):  \n",
    "                        (id,(commentLength,sentiment(body),timeSince,score,subreddit)))\n",
    "                      .persist(StorageLevel.MEMORY_AND_DISK_SER)\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "rRDDtscls.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "rRDDtscls.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.0 Set up logistic regression inputs with OHE features for categorical variable subredddit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate label from score using srDigestR and create rawData RDD in proper format:  (label, non-categorical variables, categorical variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def label(score, subreddit, percentMap):\n",
    "    if score <= percentMap[subreddit][0]: return 0\n",
    "    else: return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format of rRDDtscls:  (id,(commentLength,posNegDiff,timeSince,score,subreddit)))\n",
    "\n",
    "Format of rawData is a tuple:  (label, (0,commentLength), (1,posNegDiff), (2,timeSince), subreddit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def tupToString(tup):\n",
    "    return ','.join([str(item) for item in tup])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rawData = (rRDDtscls.map(lambda (id,(commentLength,posNegDiff,timeSince,score,subreddit)):  \n",
    "                    (label(score,subreddit,srDigestR), (0,commentLength), (1,posNegDiff), (2,timeSince), subreddit))\n",
    "                    .persist(StorageLevel.MEMORY_AND_DISK_SER)\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "type(rawData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "rawData.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "rawData.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into training, validation and test. Takes about 1-2 minutes for ~5 MB data set, greatly reduced from 85 MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights = [.8, .1, .1]\n",
    "seed = 42\n",
    "# Use randomSplit with weights and seed\n",
    "rawTrainData, rawValData, rawTestData = rawData.randomSplit(weights, seed)\n",
    "\n",
    "# Cache the data\n",
    "rawTrainData.persist(StorageLevel.MEMORY_AND_DISK_SER)\n",
    "rawValData.persist(StorageLevel.MEMORY_AND_DISK_SER)\n",
    "rawTestData.persist(StorageLevel.MEMORY_AND_DISK_SER)\n",
    "\n",
    "# These counts are expensive:  ~1 hour for 10 GB input data.\n",
    "# nAll = rawData.count()\n",
    "# nTrain = rawTrainData.count()\n",
    "# nVal = rawValData.count()\n",
    "# nTest = rawTestData.count()\n",
    "# print nTrain, nVal, nTest, nTrain + nVal + nTest, nAll\n",
    "\n",
    "# print rawData.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create one hot encoding dictionary.\n",
    "\n",
    "I DON'T NEED ML LAB 4 createOneHotDict BECAUSE I ALREADY HAVE A LIST OF SUBREDDITS IN srDigestR.  Just need to pull keys out of this dict to create my OHEdict. NOTE that my OHEdict is already shifted by 3 to avoid collision with non-categorical variables.\n",
    "\n",
    "<B> NEED TO FIX OHEdict:  should only pull categories out of TRAINING set, NOT entire set.  Otherwise it's cheating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "OHEdict = {(0, v): k+3 for (k, v) in enumerate(srDigestR)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "print OHEdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am getting all 1's as predicted values when I run using all subreddits.   I suspect that this is because I just don't have enough data.  The only subreddits that have more than 10,000 comments are programming, politics and reddit.com.  I am hard-coding this to NOT use OHEdict for now.  Instead, I am manually creating my dictionary so that the encoding will be\n",
    "\n",
    "programming [1 0 0]\n",
    "politics [0 1 0]\n",
    "reddit.com [0 0 1]\n",
    "other [0 0 0]\n",
    "\n",
    "Later when I scale up, I will re-set values in the variable \"subreddit\" to have only values that are above a certain threshold in number of comments.  I won't know the right threshold until I read in all the data and look at it.  There will probably be thousands of subreddits with too few comments to do regression on.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def createOHEMap(sr):\n",
    "    if sr == u'leagueoflegends': return (3,1)\n",
    "    elif sr == u'pics' : return (4,1)\n",
    "    elif sr == u'politics' : return (5,1)\n",
    "    else: return (5,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "OHEdict[(0,'politics')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create OHETrainData."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createLabeledPoint(point,numFeats):\n",
    "    label = point[0]\n",
    "    feats = point[1:]\n",
    "    sv = SparseVector(numFeats, feats)\n",
    "    # print sv\n",
    "    return LabeledPoint(label, sv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "len(OHEdict)+3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "createLabeledPoint((1, (0, 36), (1, -11.0), (2, 811), (25, 1)),   len(OHEdict+3  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# numFeats = len(OHEdict)+3\n",
    "numFeats = 6\n",
    "OHETrainData = (rawTrainData.map(lambda (label, t1, t2, t3, sr):\n",
    "                                       (label, t1, t2, t3, createOHEMap(sr) ))\n",
    "                            .map(lambda point:  createLabeledPoint(point, numFeats))\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OHETrainData.persist(StorageLevel.MEMORY_AND_DISK_SER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "OHETrainData.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Create OHEValData and OHETestData; skip for now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "OHEValData = (rawValData.map(lambda (label, t1, t2, t3, sr):\n",
    "                                       (label, t1, t2, t3, (OHEdict[(0,sr)], 1) ))\n",
    "                            .map(lambda point:  createLabeledPoint(point, numFeats))\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "OHETestData = (rawTestData.map(lambda (label, t1, t2, t3, sr):\n",
    "                                       (label, t1, t2, t3, (OHEdict[(0,sr)], 1) ))\n",
    "                            .map(lambda point:  createLabeledPoint(point, numFeats))\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.0 Run logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fixed hyperparameters\n",
    "numIters = 50\n",
    "stepSize = 10.\n",
    "regParam = 1e-6\n",
    "regType = 'l2'\n",
    "includeIntercept = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run logistic regression.\n",
    "\n",
    "<b> LATER, ADD VALIDATION STEP:  ITERATE OVER HYPERPARAMETERS TO FIND BEST COMBINATION."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model0 = LogisticRegressionWithSGD.train(OHETrainData, iterations = numIters, step = stepSize,\n",
    "                                        regParam = regParam, regType = regType,\n",
    "                                        intercept = includeIntercept)\n",
    "sortedWeights = sorted(model0.weights)\n",
    "# print sortedWeights[:5], model0.intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "print sortedWeights, model0.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print model0.weights, model0.intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 9.0 Evaluate results using test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Calculate accuracy = number of correctly classified examples / total number of examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model0TotalCorrect = OHETrainData.map(lambda point:  1 if model0.predict(point.features) == point.label else 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print model0TotalCorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OHETrainDataCount = OHETrainData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print OHETrainDataCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model0Accuracy = float(model0TotalCorrect) / float(OHETrainData.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print model0Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> TO DO:  Calculate ROC AUC (receiver operating characteristic - area under curve).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>TO DO:  calculate confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainingPredictionsAuto=(OHETrainData\n",
    "                     .map(lambda lpoint: model0.predict(lpoint.features))\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print trainingPredictionsAuto.take(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numNotNinesAuto = trainingPredictionsAuto.filter(lambda P: 1-P > 0.001).count() # how many predictions are not ~= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print numNotNinesAuto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print trainingPredictionsAuto.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model with default values.  Any better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modelDefault = LogisticRegressionWithSGD.train(OHETrainData, iterations = numIters,\n",
    "                                        intercept = includeIntercept)\n",
    "sortedWeights = sorted(modelDefault.weights)\n",
    "# print sortedWeights[:5], model0.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print sortedWeights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print modelDefault.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainingPredDefault=(OHETrainData\n",
    "                     .map(lambda lpoint: modelDefault.predict(lpoint.features))\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print trainingPredDefault.take(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print trainingPredDefault.filter(lambda P: 1-P > 0.001).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def bucketFeatByCount(featCount):\n",
    "    \"\"\"Bucket the counts by powers of two.\"\"\"\n",
    "    for i in range(11):\n",
    "        size = 2 ** i\n",
    "        if featCount <= size:\n",
    "            return size\n",
    "    return -1\n",
    "\n",
    "featCounts = (OHETrainData\n",
    "              .flatMap(lambda lp: lp.features.indices)\n",
    "              .map(lambda x: (x, 1))\n",
    "              .reduceByKey(lambda x, y: x + y))\n",
    "featCountsBuckets = (featCounts\n",
    "                     .map(lambda x: (bucketFeatByCount(x[1]), 1))\n",
    "                     .filter(lambda (k, v): k != -1)\n",
    "                     .reduceByKey(lambda x, y: x + y)\n",
    "                     .collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "x, y = zip(*featCountsBuckets)\n",
    "x, y = np.log(x), np.log(y)\n",
    "\n",
    "def preparePlot(xticks, yticks, figsize=(10.5, 6), hideLabels=False, gridColor='#999999',\n",
    "                gridWidth=1.0):\n",
    "    \"\"\"Template for generating the plot layout.\"\"\"\n",
    "    plt.close()\n",
    "    fig, ax = plt.subplots(figsize=figsize, facecolor='white', edgecolor='white')\n",
    "    ax.axes.tick_params(labelcolor='#999999', labelsize='10')\n",
    "    for axis, ticks in [(ax.get_xaxis(), xticks), (ax.get_yaxis(), yticks)]:\n",
    "        axis.set_ticks_position('none')\n",
    "        axis.set_ticks(ticks)\n",
    "        axis.label.set_color('#999999')\n",
    "        if hideLabels: axis.set_ticklabels([])\n",
    "    plt.grid(color=gridColor, linewidth=gridWidth, linestyle='-')\n",
    "    map(lambda position: ax.spines[position].set_visible(False), ['bottom', 'top', 'left', 'right'])\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "labelsAndScores = OHEValData.map(lambda lp:\n",
    "                                            (lp.label, getP(lp.features, model0.weights, model0.intercept)))\n",
    "labelsAndWeights = labelsAndScores.collect()\n",
    "labelsAndWeights.sort(key=lambda (k, v): v, reverse=True)\n",
    "labelsByWeight = np.array([k for (k, v) in labelsAndWeights])\n",
    "\n",
    "length = labelsByWeight.size\n",
    "truePositives = labelsByWeight.cumsum()\n",
    "numPositive = truePositives[-1]\n",
    "falsePositives = np.arange(1.0, length + 1, 1.) - truePositives\n",
    "\n",
    "truePositiveRate = truePositives / numPositive\n",
    "falsePositiveRate = falsePositives / (length - numPositive)\n",
    "\n",
    "# Generate layout and plot data\n",
    "fig, ax = preparePlot(np.arange(0., 1.1, 0.1), np.arange(0., 1.1, 0.1))\n",
    "ax.set_xlim(-.05, 1.05), ax.set_ylim(-.05, 1.05)\n",
    "ax.set_ylabel('True Positive Rate (Sensitivity)')\n",
    "ax.set_xlabel('False Positive Rate (1 - Specificity)')\n",
    "plt.plot(falsePositiveRate, truePositiveRate, color='#8cbfd0', linestyle='-', linewidth=3.)\n",
    "plt.plot((0., 1.), (0., 1.), linestyle='--', color='#d6ebf2', linewidth=2.)  # Baseline model\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
