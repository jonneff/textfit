{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    " \n",
    "# Path for spark source folder\n",
    "os.environ['SPARK_HOME'] = \"/Users/jonneff/spark-1.4.0-bin-hadoop2.6\"\n",
    " \n",
    "# Append pyspark to Python Path\n",
    "sys.path.append(\"/Users/jonneff/spark-1.4.0-bin-hadoop2.6/python\")\n",
    " \n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "# Load in the testing code and check to see if your answer is correct\n",
    "# If incorrect it will report back '1 test failed' for each failed test\n",
    "# Make sure to rerun any cell you change before trying the test again\n",
    "from test_helper import Test\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tdigest import TDigest\n",
    "from numpy.random import random\n",
    "from operator import add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.0 Read data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in data.  For now, on local filesystem.  LATER, ADD CODE TO READ FROM S3.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = sqlContext.read.json(\"/Users/jonneff/Desktop/DE/RC_2007-10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150429"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('archived', 'boolean'),\n",
       " ('author', 'string'),\n",
       " ('author_flair_css_class', 'string'),\n",
       " ('author_flair_text', 'string'),\n",
       " ('body', 'string'),\n",
       " ('controversiality', 'bigint'),\n",
       " ('created_utc', 'string'),\n",
       " ('distinguished', 'string'),\n",
       " ('downs', 'bigint'),\n",
       " ('edited', 'string'),\n",
       " ('gilded', 'bigint'),\n",
       " ('id', 'string'),\n",
       " ('link_id', 'string'),\n",
       " ('name', 'string'),\n",
       " ('parent_id', 'string'),\n",
       " ('retrieved_on', 'bigint'),\n",
       " ('score', 'bigint'),\n",
       " ('score_hidden', 'boolean'),\n",
       " ('subreddit', 'string'),\n",
       " ('subreddit_id', 'string'),\n",
       " ('ups', 'bigint')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- archived: boolean (nullable = true)\n",
      " |-- author: string (nullable = true)\n",
      " |-- author_flair_css_class: string (nullable = true)\n",
      " |-- author_flair_text: string (nullable = true)\n",
      " |-- body: string (nullable = true)\n",
      " |-- controversiality: long (nullable = true)\n",
      " |-- created_utc: string (nullable = true)\n",
      " |-- distinguished: string (nullable = true)\n",
      " |-- downs: long (nullable = true)\n",
      " |-- edited: string (nullable = true)\n",
      " |-- gilded: long (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- link_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- parent_id: string (nullable = true)\n",
      " |-- retrieved_on: long (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      " |-- score_hidden: boolean (nullable = true)\n",
      " |-- subreddit: string (nullable = true)\n",
      " |-- subreddit_id: string (nullable = true)\n",
      " |-- ups: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(archived=True, author=u'bostich', author_flair_css_class=None, author_flair_text=None, body=u'test', controversiality=0, created_utc=u'1192450635', distinguished=None, downs=0, edited=u'false', gilded=0, id=u'c0299an', link_id=u't3_5yba3', name=u't1_c0299an', parent_id=u't3_5yba3', retrieved_on=1427426409, score=1, score_hidden=False, subreddit=u'reddit.com', subreddit_id=u't5_6', ups=1)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(archived=True, author=u'bostich', author_flair_css_class=None, author_flair_text=None, body=u'test', controversiality=0, created_utc=u'1192450635', distinguished=None, downs=0, edited=u'false', gilded=0, id=u'c0299an', link_id=u't3_5yba3', name=u't1_c0299an', parent_id=u't3_5yba3', retrieved_on=1427426409, score=1, score_hidden=False, subreddit=u'reddit.com', subreddit_id=u't5_6', ups=1)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Find minimum comment timestamp for each post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each post link_id, find minimum created_utc timestamp [can't trust data is ordered by time] and store in key-value pair (pair RDD) {link_id: min_created_utc},  (Plan B:  set up API to get timestamp for all posts in Reddit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "redditRDD = df.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(archived=True, author=u'bostich', author_flair_css_class=None, author_flair_text=None, body=u'test', controversiality=0, created_utc=u'1192450635', distinguished=None, downs=0, edited=u'false', gilded=0, id=u'c0299an', link_id=u't3_5yba3', name=u't1_c0299an', parent_id=u't3_5yba3', retrieved_on=1427426409, score=1, score_hidden=False, subreddit=u'reddit.com', subreddit_id=u't5_6', ups=1)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redditRDD.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rRDD = df.map(lambda r: (r.id, (r.body, int(r.created_utc), r.link_id, r.parent_id, int(r.score), r.subreddit, r.subreddit_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'c0299an',\n",
       "  (u'test', 1192450635, u't3_5yba3', u't3_5yba3', 1, u'reddit.com', u't5_6'))]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rRDD.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort to find minimum timestamp created_utc for each post link_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150429"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "minTimeRDD = (rRDD.map(lambda (k, v): (v[2],v[1])) # maps to (link_id, created_utc)\n",
    "                  .reduceByKey(lambda a, b:  a if a < b else b)\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u't3_5yxsg', 1193154263)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minTimeRDD.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Filter to include only extreme up and down votes (top 3% of subreddit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter to retain only records that are top or bottom 3% in comment score (upvotes-downvotes) of their subreddit.  Reduces dataset for all subsequent processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find 3% and 97% percentiles for each subreddit.  Use T-digest data structure for highly accurate approximate percentiles: \n",
    "http://dataorigami.net/blogs/napkin-folding/19055451-percentile-and-quantile-estimation-of-big-data-the-t-digest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def digest(values):\n",
    "    digest = TDigest()\n",
    "    digest.batch_update(values)\n",
    "    return digest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def digest1(value):\n",
    "    digest = TDigest()\n",
    "    digest.update(value)\n",
    "    return digest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to get at scores for each subreddit, THEN map using digest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tdigest with mapreduce doesn't work; i get something that is not a tdigest object.  Ronak suggests a solution, see project log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subredditDigestRDD = (rRDD.map(lambda (k, v):  (v[5], v[4]))\n",
    "                        .map(lambda (k, v): (k, digest1(v)))  \n",
    "                        .reduceByKey(lambda a, b:  a + b)\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(subredditDigestRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subredditDigest = subredditDigestRDD.collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tdigest.tdigest.TDigest"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(subredditDigest['politics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.3696052631578945"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subredditDigest['politics'].percentile(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.797647058823536"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subredditDigest['politics'].percentile(97)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alyssa's values for politics were (-6,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map reddit RDD to key,value pair (subreddit, [score])\n",
    "Reduce by adding list b to the end of list a.  This creates a list of scores by subreddit.\n",
    "Create a t-digest for each subreddit representing distribution of scores.\n",
    "\n",
    "FOLLOWING CODE DOESN'T WORK BECAUSE SPARK WON'T LET ME CAST V[4] AS A LIST AND WON'T RECOGNIZE [v[4]] as a list either."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "subredditDigestRDD = (rRDD.map(lambda (k, v): (v[5], list(v[4])))\n",
    "                         .reduceByKey(lambda a, b:  a.extend(b))\n",
    "                      )\n",
    "                       #   .map(lambda (k, v):  (k,digest(v)))\n",
    "                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 17.0 failed 1 times, most recent failure: Lost task 0.0 in stage 17.0 (TID 31, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/jonneff/spark-1.4.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/Users/jonneff/spark-1.4.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/jonneff/spark-1.4.0-bin-hadoop2.6/python/pyspark/rdd.py\", line 2318, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/Users/jonneff/spark-1.4.0-bin-hadoop2.6/python/pyspark/rdd.py\", line 2318, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/Users/jonneff/spark-1.4.0-bin-hadoop2.6/python/pyspark/rdd.py\", line 304, in func\n    return f(iterator)\n  File \"/Users/jonneff/spark-1.4.0-bin-hadoop2.6/python/pyspark/rdd.py\", line 1746, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/Users/jonneff/spark-1.4.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/shuffle.py\", line 266, in mergeValues\n    for k, v in iterator:\n  File \"<ipython-input-25-8318ed08283d>\", line 1, in <lambda>\nTypeError: 'int' object is not iterable\n\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:138)\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:179)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:97)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:244)\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:315)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:244)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:70)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:70)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1266)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1257)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1256)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1256)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1450)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1411)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-771e177731da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0msubredditDigestRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/jonneff/spark-1.4.0-bin-hadoop2.6/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1264\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1265\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1267\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonneff/spark-1.4.0-bin-hadoop2.6/python/pyspark/context.pyc\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m         port = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions,\n\u001b[0;32m--> 881\u001b[0;31m                                           allowLocal)\n\u001b[0m\u001b[1;32m    882\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonneff/spark-1.4.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[0;32m--> 538\u001b[0;31m                 self.target_id, self.name)\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonneff/spark-1.4.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    298\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    299\u001b[0m                     \u001b[0;34m'An error occurred while calling {0}{1}{2}.\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     format(target_id, '.', name), value)\n\u001b[0m\u001b[1;32m    301\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 17.0 failed 1 times, most recent failure: Lost task 0.0 in stage 17.0 (TID 31, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/jonneff/spark-1.4.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/Users/jonneff/spark-1.4.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/jonneff/spark-1.4.0-bin-hadoop2.6/python/pyspark/rdd.py\", line 2318, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/Users/jonneff/spark-1.4.0-bin-hadoop2.6/python/pyspark/rdd.py\", line 2318, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/Users/jonneff/spark-1.4.0-bin-hadoop2.6/python/pyspark/rdd.py\", line 304, in func\n    return f(iterator)\n  File \"/Users/jonneff/spark-1.4.0-bin-hadoop2.6/python/pyspark/rdd.py\", line 1746, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/Users/jonneff/spark-1.4.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/shuffle.py\", line 266, in mergeValues\n    for k, v in iterator:\n  File \"<ipython-input-25-8318ed08283d>\", line 1, in <lambda>\nTypeError: 'int' object is not iterable\n\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:138)\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:179)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:97)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:244)\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:315)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:244)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:70)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:70)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1266)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1257)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1256)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1256)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1450)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1411)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n"
     ]
    }
   ],
   "source": [
    "print subredditDigestRDD.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 12.0 failed 1 times, most recent failure: Lost task 0.0 in stage 12.0 (TID 24, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/jonneff/spark-1.4.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/Users/jonneff/spark-1.4.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/jonneff/spark-1.4.0-bin-hadoop2.6/python/pyspark/rdd.py\", line 2318, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/Users/jonneff/spark-1.4.0-bin-hadoop2.6/python/pyspark/rdd.py\", line 2318, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/Users/jonneff/spark-1.4.0-bin-hadoop2.6/python/pyspark/rdd.py\", line 304, in func\n    return f(iterator)\n  File \"/Users/jonneff/spark-1.4.0-bin-hadoop2.6/python/pyspark/rdd.py\", line 1746, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/Users/jonneff/spark-1.4.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/shuffle.py\", line 268, in mergeValues\n    d[k] = comb(d[k], v) if k in d else creator(v)\n  File \"<ipython-input-18-c9f7211b77d7>\", line 2, in <lambda>\nAttributeError: 'NoneType' object has no attribute 'extend'\n\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:138)\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:179)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:97)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:244)\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:315)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:244)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:70)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:70)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1266)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1257)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1256)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1256)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1450)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1411)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-68085f0b4aaa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msubredditDigest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubredditDigestRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAsMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Turn RDD into dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/jonneff/spark-1.4.0-bin-hadoop2.6/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mcollectAsMap\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1486\u001b[0m         \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1487\u001b[0m         \"\"\"\n\u001b[0;32m-> 1488\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1490\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonneff/spark-1.4.0-bin-hadoop2.6/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    743\u001b[0m         \"\"\"\n\u001b[1;32m    744\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 745\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    746\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonneff/spark-1.4.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[0;32m--> 538\u001b[0;31m                 self.target_id, self.name)\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonneff/spark-1.4.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    298\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    299\u001b[0m                     \u001b[0;34m'An error occurred while calling {0}{1}{2}.\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     format(target_id, '.', name), value)\n\u001b[0m\u001b[1;32m    301\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 12.0 failed 1 times, most recent failure: Lost task 0.0 in stage 12.0 (TID 24, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/jonneff/spark-1.4.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/Users/jonneff/spark-1.4.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/jonneff/spark-1.4.0-bin-hadoop2.6/python/pyspark/rdd.py\", line 2318, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/Users/jonneff/spark-1.4.0-bin-hadoop2.6/python/pyspark/rdd.py\", line 2318, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/Users/jonneff/spark-1.4.0-bin-hadoop2.6/python/pyspark/rdd.py\", line 304, in func\n    return f(iterator)\n  File \"/Users/jonneff/spark-1.4.0-bin-hadoop2.6/python/pyspark/rdd.py\", line 1746, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/Users/jonneff/spark-1.4.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/shuffle.py\", line 268, in mergeValues\n    d[k] = comb(d[k], v) if k in d else creator(v)\n  File \"<ipython-input-18-c9f7211b77d7>\", line 2, in <lambda>\nAttributeError: 'NoneType' object has no attribute 'extend'\n\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:138)\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:179)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:97)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:244)\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:315)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:244)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:70)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:70)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1266)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1257)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1256)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1256)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1450)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1411)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n"
     ]
    }
   ],
   "source": [
    "subredditDigest = subredditDigestRDD.collectAsMap() # Turn RDD into dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "polPerDigest = subredditDigest[u'politics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "builtin_function_or_method"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(polPerDigest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter records to retain only top and bottom 3% of comment scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 Filter out exclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out exclusions.  Further reduces dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.0  Calculate timeSince"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate timeSince based on created_utc and min_created_utc from pair RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.0  Calculate commentLength"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean comment body and calculate commentLength."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.0 Run sentiment analysis and calculate posNegDiff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.0 Set up OHE for subreddit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.0 (Deal with sparse matrices from OHE if necessary; skip for now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.0 Run logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATABASE = 'reddit'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "engine = create_engine(\"mysql+pymysql://root@localhost/\" + str(DATABASE))\n",
    "con = engine.connect()\n",
    "dataid = 1022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "redditDB = con.execute(\"SELECT * FROM comments6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reddit = pd.DataFrame(redditDB.fetchall())\n",
    "reddit.columns = redditDB.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>commentAuthor</th>\n",
       "      <th>commentCreated</th>\n",
       "      <th>commentID</th>\n",
       "      <th>commentLink</th>\n",
       "      <th>commentScore</th>\n",
       "      <th>postID</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>postAuthor</th>\n",
       "      <th>postBody</th>\n",
       "      <th>postCreated</th>\n",
       "      <th>...</th>\n",
       "      <th>NegRaw</th>\n",
       "      <th>vNegRaw</th>\n",
       "      <th>vPos</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Neg</th>\n",
       "      <th>vNeg</th>\n",
       "      <th>posNegRatio</th>\n",
       "      <th>posNegDiff</th>\n",
       "      <th>commentBody2</th>\n",
       "      <th>commentLengthSW</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>     Ajunadeeps</td>\n",
       "      <td> 1432770000</td>\n",
       "      <td> crmzgnx</td>\n",
       "      <td> http://www.reddit.com/r/leagueoflegends/commen...</td>\n",
       "      <td> 586</td>\n",
       "      <td> 37ihzo</td>\n",
       "      <td> leagueoflegends</td>\n",
       "      <td> DBlackjack21</td>\n",
       "      <td> http://gfycat.com/HopefulGleefulHoiho</td>\n",
       "      <td> 1432770000</td>\n",
       "      <td>...</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> None</td>\n",
       "      <td> None</td>\n",
       "      <td> None</td>\n",
       "      <td> None</td>\n",
       "      <td>  0</td>\n",
       "      <td> 0.600000</td>\n",
       "      <td>                     Best backdoor since gay porn.</td>\n",
       "      <td>  5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>   DBlackjack21</td>\n",
       "      <td> 1432780000</td>\n",
       "      <td> crn46mg</td>\n",
       "      <td> http://www.reddit.com/r/leagueoflegends/commen...</td>\n",
       "      <td> 441</td>\n",
       "      <td> 37ihzo</td>\n",
       "      <td> leagueoflegends</td>\n",
       "      <td> DBlackjack21</td>\n",
       "      <td> http://gfycat.com/HopefulGleefulHoiho</td>\n",
       "      <td> 1432770000</td>\n",
       "      <td>...</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> None</td>\n",
       "      <td> None</td>\n",
       "      <td> None</td>\n",
       "      <td> None</td>\n",
       "      <td>  0</td>\n",
       "      <td> 0.200000</td>\n",
       "      <td> [Objection!](http://gfycat.com/OptimisticSkinn...</td>\n",
       "      <td>  5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td> ThePowerOfAura</td>\n",
       "      <td> 1432780000</td>\n",
       "      <td> crn58hk</td>\n",
       "      <td> http://www.reddit.com/r/leagueoflegends/commen...</td>\n",
       "      <td> 325</td>\n",
       "      <td> 37ihzo</td>\n",
       "      <td> leagueoflegends</td>\n",
       "      <td> DBlackjack21</td>\n",
       "      <td> http://gfycat.com/HopefulGleefulHoiho</td>\n",
       "      <td> 1432770000</td>\n",
       "      <td>...</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> None</td>\n",
       "      <td> None</td>\n",
       "      <td> None</td>\n",
       "      <td> None</td>\n",
       "      <td>  0</td>\n",
       "      <td> 0.166667</td>\n",
       "      <td> Dude and I thought Sion was easy to play, clea...</td>\n",
       "      <td> 18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>       SkyhuntL</td>\n",
       "      <td> 1432790000</td>\n",
       "      <td> crn9iqq</td>\n",
       "      <td> http://www.reddit.com/r/leagueoflegends/commen...</td>\n",
       "      <td> 125</td>\n",
       "      <td> 37ihzo</td>\n",
       "      <td> leagueoflegends</td>\n",
       "      <td> DBlackjack21</td>\n",
       "      <td> http://gfycat.com/HopefulGleefulHoiho</td>\n",
       "      <td> 1432770000</td>\n",
       "      <td>...</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> None</td>\n",
       "      <td> None</td>\n",
       "      <td> None</td>\n",
       "      <td> None</td>\n",
       "      <td>NaN</td>\n",
       "      <td> 0.000000</td>\n",
       "      <td>                   Or a driving license, that is. </td>\n",
       "      <td>  7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>  offlightsedge</td>\n",
       "      <td> 1432790000</td>\n",
       "      <td> crnc5oz</td>\n",
       "      <td> http://www.reddit.com/r/leagueoflegends/commen...</td>\n",
       "      <td>  33</td>\n",
       "      <td> 37ihzo</td>\n",
       "      <td> leagueoflegends</td>\n",
       "      <td> DBlackjack21</td>\n",
       "      <td> http://gfycat.com/HopefulGleefulHoiho</td>\n",
       "      <td> 1432770000</td>\n",
       "      <td>...</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> None</td>\n",
       "      <td> None</td>\n",
       "      <td> None</td>\n",
       "      <td> None</td>\n",
       "      <td>  0</td>\n",
       "      <td> 0.333333</td>\n",
       "      <td> Preparing for a wide turn like that looks like...</td>\n",
       "      <td> 12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    commentAuthor  commentCreated commentID  \\\n",
       "0      Ajunadeeps      1432770000   crmzgnx   \n",
       "1    DBlackjack21      1432780000   crn46mg   \n",
       "2  ThePowerOfAura      1432780000   crn58hk   \n",
       "3        SkyhuntL      1432790000   crn9iqq   \n",
       "4   offlightsedge      1432790000   crnc5oz   \n",
       "\n",
       "                                         commentLink  commentScore  postID  \\\n",
       "0  http://www.reddit.com/r/leagueoflegends/commen...           586  37ihzo   \n",
       "1  http://www.reddit.com/r/leagueoflegends/commen...           441  37ihzo   \n",
       "2  http://www.reddit.com/r/leagueoflegends/commen...           325  37ihzo   \n",
       "3  http://www.reddit.com/r/leagueoflegends/commen...           125  37ihzo   \n",
       "4  http://www.reddit.com/r/leagueoflegends/commen...            33  37ihzo   \n",
       "\n",
       "         subreddit    postAuthor                               postBody  \\\n",
       "0  leagueoflegends  DBlackjack21  http://gfycat.com/HopefulGleefulHoiho   \n",
       "1  leagueoflegends  DBlackjack21  http://gfycat.com/HopefulGleefulHoiho   \n",
       "2  leagueoflegends  DBlackjack21  http://gfycat.com/HopefulGleefulHoiho   \n",
       "3  leagueoflegends  DBlackjack21  http://gfycat.com/HopefulGleefulHoiho   \n",
       "4  leagueoflegends  DBlackjack21  http://gfycat.com/HopefulGleefulHoiho   \n",
       "\n",
       "   postCreated       ...        NegRaw  vNegRaw  vPos   Pos   Neg  vNeg  \\\n",
       "0   1432770000       ...             0        0  None  None  None  None   \n",
       "1   1432770000       ...             0        0  None  None  None  None   \n",
       "2   1432770000       ...             0        0  None  None  None  None   \n",
       "3   1432770000       ...             0        0  None  None  None  None   \n",
       "4   1432770000       ...             0        0  None  None  None  None   \n",
       "\n",
       "   posNegRatio  posNegDiff                                       commentBody2  \\\n",
       "0            0    0.600000                      Best backdoor since gay porn.   \n",
       "1            0    0.200000  [Objection!](http://gfycat.com/OptimisticSkinn...   \n",
       "2            0    0.166667  Dude and I thought Sion was easy to play, clea...   \n",
       "3          NaN    0.000000                    Or a driving license, that is.    \n",
       "4            0    0.333333  Preparing for a wide turn like that looks like...   \n",
       "\n",
       "   commentLengthSW  \n",
       "0                5  \n",
       "1                5  \n",
       "2               18  \n",
       "3                7  \n",
       "4               12  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     253519.000000\n",
       "mean       32795.809746\n",
       "std        57652.954846\n",
       "min            0.000000\n",
       "25%         9216.500000\n",
       "50%        24702.000000\n",
       "75%        43076.000000\n",
       "max      3279309.000000\n",
       "Name: timeSince, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit['timeSince'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    287829.000000\n",
       "mean          6.800795\n",
       "std          34.962045\n",
       "min        -416.000000\n",
       "25%           0.000000\n",
       "50%           1.000000\n",
       "75%           4.000000\n",
       "max        2143.000000\n",
       "Name: commentScore, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit['commentScore'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "redditOrig = reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "redditA = reddit[['commentID', 'subreddit', 'commentScore', 'timeSince', 'commentLength', 'posNegDiff']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def perc(subreddit):\n",
    "    return (round(np.percentile(reddit[reddit['subreddit'] == subreddit]['commentScore'], 3, axis = 0)), round(np.percentile(reddit[reddit['subreddit'] == subreddit]['commentScore'], 97, axis = 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LoLper = perc('leagueoflegends')\n",
    "GGper = perc('GirlGamers')\n",
    "picsper = perc('pics')\n",
    "polper = perc('politics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-7.0, 66.0)\n",
      "(0.0, 26.0)\n",
      "(-5.0, 60.0)\n",
      "(-6.0, 24.0)\n"
     ]
    }
   ],
   "source": [
    "print(LoLper)\n",
    "print(GGper)\n",
    "print(picsper)\n",
    "print(polper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score(subreddit, per):\n",
    "    data = redditA[redditA['subreddit'] == subreddit]\n",
    "    data['scoreC'] = np.nan\n",
    "    data['scoreC'][data['commentScore'] <= per[0]] = 0\n",
    "    data['scoreC'][data['commentScore'] >= per[1]] = 1\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/anaconda/lib/python2.7/site-packages/IPython/kernel/__main__.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n",
      "/anaconda/anaconda/lib/python2.7/site-packages/IPython/kernel/__main__.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda/anaconda/lib/python2.7/site-packages/pandas/core/generic.py:3368: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._update_inplace(new_data)\n",
      "/anaconda/anaconda/lib/python2.7/site-packages/IPython/kernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "/anaconda/anaconda/lib/python2.7/site-packages/IPython/kernel/__main__.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda/anaconda/lib/python2.7/site-packages/IPython/kernel/__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from IPython.kernel.zmq import kernelapp as app\n",
      "/anaconda/anaconda/lib/python2.7/site-packages/IPython/kernel/__main__.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "LoL = score('leagueoflegends', LoLper)\n",
    "GG = score('GirlGamers', GGper)\n",
    "pic = score('pics', picsper)\n",
    "pol = score('politics', polper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    6256.000000\n",
      "mean        0.468191\n",
      "std         0.499027\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         1.000000\n",
      "max         1.000000\n",
      "Name: scoreC, dtype: float64\n",
      "count    499.000000\n",
      "mean       0.386774\n",
      "std        0.487500\n",
      "min        0.000000\n",
      "25%        0.000000\n",
      "50%        0.000000\n",
      "75%        1.000000\n",
      "max        1.000000\n",
      "Name: scoreC, dtype: float64\n",
      "count    5535.000000\n",
      "mean        0.487986\n",
      "std         0.499901\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         1.000000\n",
      "max         1.000000\n",
      "Name: scoreC, dtype: float64\n",
      "count    3763.000000\n",
      "mean        0.484985\n",
      "std         0.499841\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         1.000000\n",
      "max         1.000000\n",
      "Name: scoreC, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print LoL['scoreC'].describe()\n",
    "print GG['scoreC'].describe()\n",
    "print pic['scoreC'].describe()\n",
    "print pol['scoreC'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "redditB = pd.concat([LoL, GG, pic, pol])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>commentScore</th>\n",
       "      <th>timeSince</th>\n",
       "      <th>commentLength</th>\n",
       "      <th>posNegDiff</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scoreC</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td> -16.081737</td>\n",
       "      <td> 20443.331973</td>\n",
       "      <td> 30.225986</td>\n",
       "      <td>-0.016371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td> 142.337997</td>\n",
       "      <td> 10461.994676</td>\n",
       "      <td> 27.414966</td>\n",
       "      <td> 0.005304</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        commentScore     timeSince  commentLength  posNegDiff\n",
       "scoreC                                                       \n",
       "0         -16.081737  20443.331973      30.225986   -0.016371\n",
       "1         142.337997  10461.994676      27.414966    0.005304"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redditB.groupby('scoreC').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'Intercept', u'C(subreddit)[T.leagueoflegends]', u'C(subreddit)[T.pics]', u'C(subreddit)[T.politics]', u'timeSince', u'commentLength', u'posNegDiff'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "y, X = dmatrices('scoreC ~ C(subreddit) + timeSince + commentLength + posNegDiff', redditB, return_type = \"dataframe\")\n",
    "print X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = X.rename(columns = {'C(subreddit)[T.leagueoflegends]': 'subLoL', \n",
    "                        'C(subreddit)[T.pics]': 'subPics', \n",
    "                        'C(subreddit)[T.politics]': 'subPol'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = np.ravel(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model = model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('model', 'w')\n",
    "cPickle.dump(model, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>     Intercept</td>\n",
       "      <td>     [0.410245670233]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>        subLoL</td>\n",
       "      <td>    [0.0583930466606]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>       subPics</td>\n",
       "      <td>     [0.244273666339]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>        subPol</td>\n",
       "      <td>     [0.100347697994]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>     timeSince</td>\n",
       "      <td> [-7.30151157008e-05]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td> commentLength</td>\n",
       "      <td> [-0.000133795596042]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>    posNegDiff</td>\n",
       "      <td>    [0.0322472638634]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0                     1\n",
       "0      Intercept      [0.410245670233]\n",
       "1         subLoL     [0.0583930466606]\n",
       "2        subPics      [0.244273666339]\n",
       "3         subPol      [0.100347697994]\n",
       "4      timeSince  [-7.30151157008e-05]\n",
       "5  commentLength  [-0.000133795596042]\n",
       "6     posNegDiff     [0.0322472638634]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(zip(X.columns, np.transpose(model.coef_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  1.  1. ...,  0.  0.  1.]\n"
     ]
    }
   ],
   "source": [
    "predicted = model.predict(X_test)\n",
    "print predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.26103584  0.73896416]\n",
      " [ 0.45698479  0.54301521]\n",
      " [ 0.49617659  0.50382341]\n",
      " ..., \n",
      " [ 0.62978508  0.37021492]\n",
      " [ 0.79438198  0.20561802]\n",
      " [ 0.27561929  0.72438071]]\n"
     ]
    }
   ],
   "source": [
    "probs = model.predict_proba(X_test)\n",
    "print probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.646433632499\n",
      "0.709111871693\n"
     ]
    }
   ],
   "source": [
    "print metrics.accuracy_score(y_test, predicted)\n",
    "print metrics.roc_auc_score(y_test, probs[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1400  806]\n",
      " [ 691 1337]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.67      0.63      0.65      2206\n",
      "        1.0       0.62      0.66      0.64      2028\n",
      "\n",
      "avg / total       0.65      0.65      0.65      4234\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print metrics.confusion_matrix(y_test, predicted)\n",
    "print metrics.classification_report(y_test, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.42780781,  0.57219219]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_proba(np.array([1, 0, 0, 0, 2*60*60, 30, .001]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.85967768,  0.14032232]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_proba(np.array([1, 0, 0, 0, 10*60*60, 10, -.1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.38821503,  0.61178497]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_proba(np.array([0, 0, 1, 0, 1*60*60, 10, 2]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
