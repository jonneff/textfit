{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    " \n",
    "# Path for spark source folder\n",
    "os.environ['SPARK_HOME'] = \"/usr/local/spark\"\n",
    "\n",
    "# Append pyspark to Python Path\n",
    "sys.path.append(\"/usr/local/spark/python\")\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "# Load in the testing code and check to see if your answer is correct\n",
    "# If incorrect it will report back '1 test failed' for each failed test\n",
    "# Make sure to rerun any cell you change before trying the test again\n",
    "from test_helper import Test\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "# Append afinn to Python Path and import afinn.  Used for pulling data from percentiles.\n",
    "sys.path.append(\"/usr/local/lib/python2.7/dist-packages/afinn\")\n",
    "from afinn import Afinn\n",
    "\n",
    "# Stuff for logistic regression\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "from pyspark.mllib.linalg import SparseVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sys.path.append(\"/usr/local/lib/python2.7/dist-packages\")\n",
    "from tdigest import TDigest\n",
    "from numpy.random import random\n",
    "from operator import add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.0 Read data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in data.  For now, on local filesystem.  \n",
    "\n",
    "<B>ADD CODE TO READ FROM S3.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "df = sqlContext.read.json(\"/home/ubuntu/RC_2007-10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = sqlContext.read.json(\"s3n://reddit-comments/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Find minimum comment timestamp for each post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each post link_id, find minimum created_utc timestamp [can't trust data is ordered by time] and store in key-value pair (pair RDD) {link_id: min_created_utc},  (Plan B:  set up API to get timestamp for all posts in Reddit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<B>I'M KEEPING TOO MUCH DATA.  AT THIS STEP, AND SUBSEQUENT STEPS, THROW OUT DATA I'M NOT USING."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rRDD = df.map(lambda r: (r.id, (r.body, int(r.created_utc), r.link_id, r.parent_id, int(r.score), r.subreddit, r.subreddit_id)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "rRDD.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort to find minimum timestamp created_utc for each post link_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "rRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "minTimeRDD = (rRDD.map(lambda (k, v): (v[2],v[1])) # maps to (link_id, created_utc)\n",
    "                  .reduceByKey(lambda a, b:  a if a < b else b)\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "minTimeRDD.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "minTimeRDD.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Filter to include only extreme up and down votes (top 3% of subreddit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter to retain only records that are top or bottom 3% in comment score (upvotes-downvotes) of their subreddit.  Reduces dataset for all subsequent processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find 3% and 97% percentiles for each subreddit.  Use T-digest data structure for highly accurate approximate percentiles: \n",
    "http://dataorigami.net/blogs/napkin-folding/19055451-percentile-and-quantile-estimation-of-big-data-the-t-digest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def digest1(value):\n",
    "    digest = TDigest()\n",
    "    digest.update(value)\n",
    "    return digest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to get at scores for each subreddit, THEN map using digest.\n",
    "\n",
    "\n",
    "<b>REFACTOR AND MAKE THIS RUN FASTER.  I'M CALLING DIGEST1 TOO MANY TIMES.\n",
    "\n",
    "ALSO, NEED TO SPLIT INTO TRAINING, VAL AND TEST BEFORE CREATING SUBREDDIT DIGEST ON TRAINING DATA ONLY.  OTHERWISE I'M CHEATING.\n",
    "</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subredditDigestRDD = (rRDD.map(lambda (k, v):  (v[5], v[4]))\n",
    "                        .map(lambda (k, v): (k, digest1(v)))  \n",
    "                        .reduceByKey(lambda a, b:  a + b)\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "type(subredditDigestRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating digest dictionary takes a few minutes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subredditDigest = subredditDigestRDD.collectAsMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "type(subredditDigest['politics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "subredditDigest['politics'].percentile(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "subredditDigest['politics'].percentile(97)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alyssa's values for politics were (-6.0, 24.0) rounded.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set limits for what data to include by percentile.  Use only data less than lowPercentile or greater than highPercentile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lowPercentile, highPercentile = 3, 97"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "srDigestR = {key : (round(subredditDigest[key].percentile(lowPercentile)), \n",
    "                    round(subredditDigest[key].percentile(highPercentile))) \n",
    "             for key in subredditDigest.keys()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "srDigestR['politics']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "len(srDigestR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "print srDigestR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter records to retain only top and bottom 3% of comment scores.\n",
    "\n",
    "<b>NOTE:  NEED TO BROADCAST srDigestR WHEN I MOVE TO CLUSTER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rRDDExtreme = rRDD.filter(lambda (k,v): v[4] < srDigestR[v[5]][0] or v[4] > srDigestR[v[5]][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "rRDDExtreme.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0  Calculate timeSince"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate time since post was created based on created_utc and min_created_utc from pair RDD.  In Alyssa's IPython notebook this is called timeSince.  In her R code it's called recency.  \n",
    "\n",
    "For this I need a left outer join.  For each element (k, v) in self, the resulting RDD will either contain all pairs (k, (v, w)) for w in other, or the pair (k, (v, None)) if no elements in other have key k.  I need to know if there are records in my comment dataset for which there is no \"minimum time\", as this would indicate a processing error.\n",
    "\n",
    "Map RDD to get post link_id as key, then join with minTimeRDD.\n",
    "\n",
    "<b>CAN I USE DATAFRAMES FOR ALL THIS? GETTING TIRED OF REMEMBERING WHAT v[4] IS.  </b>\n",
    "\n",
    "The only data I need for regression are:  C(subreddit) + timeSince + commentLength + posNegDiff.  Need to keep only comment id (key), score, subreddit, timeSince, comment text from this step.\n",
    "\n",
    "Format of output RDD is (id,(body,timeSince,score,subreddit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rRDDXts = (rRDDExtreme.map(lambda (k,v):  (v[2],(k,v[0],v[1],v[2],v[3],v[4],v[5],v[6])))  # pull link_id as key\n",
    "                      .leftOuterJoin(minTimeRDD) # join on link_id (post)\n",
    "                      .map(lambda (link_id,(x,min_utc)):  (x[0], (x[1],x[2]-min_utc,x[5],x[6])))\n",
    "                      .cache()\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "rRDDXts.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.0  Calculate commentLength"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean comment body and calculate commentLength.\n",
    "\n",
    "R gsub:\n",
    "gsub(pattern, replacement, x, ignore.case = FALSE, perl = FALSE, fixed = FALSE, useBytes = FALSE)\n",
    "\n",
    "Python re:\n",
    "re.sub(pattern, repl, string, count=0, flags=0).  Return the string obtained by replacing the leftmost non-overlapping occurrences of pattern in string by the replacement repl.\n",
    "\n",
    "<B>NOTE:  ALYSSA REMOVED QUOTED COMMENTS.  I REMOVING THEM ALSO BUT IN MY FIRST EXAMPLE I FOUND A \"MADE UP\" QUOTE THAT ISN'T REALLY QUOTING SOMEONE ELSE'S POST.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanup(body):\n",
    "\n",
    "\t# Recode HTML codes\n",
    "\tbody = re.sub(\"&gt;\", \">\", body)\n",
    "\tbody = re.sub(\"&lt;\", \"<\", body)\n",
    "\tbody = re.sub(\"&amp;\", \"&\", body)\n",
    "\tbody = re.sub(\"&nbsp;\", \" \", body)\n",
    "\n",
    "\t# Remove deleted\n",
    "\tbody = re.sub(\"^[deleted]$\", \"\", body)\n",
    "\n",
    "\t# Remove URL\n",
    "\tbody = re.sub(\"http[[:alnum:][:punct:]]*\", \" \", body) # url\n",
    "\n",
    "\t# Remove /r/subreddit, /u/user\n",
    "\tbody = re.sub(\"/r/[[:alnum:]]+|/u/[[:alnum:]]+\", \" \", body)\n",
    "\n",
    "\t# Remove quoted comments\n",
    "\tbody = re.sub(\"(>.*?\\\\n\\\\n)+\", \" \", body)\n",
    "\n",
    "\t# Remove control characters (\\n, \\b)\n",
    "\tbody = re.sub(\"[[:cntrl:]]\", \" \", body)\n",
    "\n",
    "\t# Remove single quotation marks (contractions)\n",
    "\tbody = re.sub(\"'\", \"\", body)\n",
    "\n",
    "\t# Remove punctuation\n",
    "\tbody = re.sub(\"[[:punct:]]\", \" \", body)\n",
    "\n",
    "\t# Replace multiple spaces with single space\n",
    "\tbody = re.sub(\"\\\\s+\", \" \", body) # Multiple spaces\n",
    "\t# body = re.sub(\"^\\\\s+\", \"\", body) # Space at the start of the string\n",
    "\t# body = re.sub(\"+\\\\s$\", \"\", body) # Space at the end of the string\n",
    "\tbody = body.strip()\n",
    "\n",
    "\t# Lower case\n",
    "\tbody = body.lower()\n",
    "\n",
    "\t# Return comment length (number of words) and body (cleaned up text)\n",
    "\treturn body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "clbody = cleanup(u\"Basically, the hospital's position amounts to:\\n\\n&gt; If she can't hold her roofies she deserves to be a**f****d and denied medical care and collection of evidence!\\n\\nNot the *most* progressive attitude...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "print len(clbody.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "print clbody"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current format of RDD:  (id,(body,timeSince,score,subreddit))\n",
    "Format of rRDDXtscl:  (id,(commentLength,body,timeSince,score,subreddit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rRDDXtscl = (rRDDXts.map(lambda (id,(body,timeSince,score,subreddit)): (id,(cleanup(body),timeSince,score,subreddit)))\n",
    "                    .map(lambda (id,(body,timeSince,score,subreddit)): (id,(len(body.split()),body,timeSince,score,subreddit)))\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.0 (Filter out exclusions if necessary; skip for now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out exclusions.  Further reduces dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.0 Run sentiment analysis and calculate posNegDiff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use AFINN model to do sentiment analysis.\n",
    "\n",
    "Finn Ã…rup Nielsen, \"A new ANEW: evaluation of a word list for sentiment analysis in microblogs\" , Proceedings of the ESWC2011 Workshop on 'Making Sense of Microposts': Big things come in small packages 718 in CEUR Workshop Proceedings: 93-98. 2011 May. Matthew Rowe, Milan Stankovic, Aba-Sah Dadzie, Mariann Hardey (editors)\n",
    "\n",
    "<B>I'M WONDERING IF CREATING AN AFINN OBJECT EACH TIME IS TAKING TOO LONG.  MIGHT WANT TO REFACTOR THIS TO A PYTHON FUNCTION WITH A DICT LOOKUP AND USE A BROADCAST VARIABLE.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentiment(body):\n",
    "    afinn = Afinn()\n",
    "    return afinn.score(body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "sentiment(\"This is utterly excellent!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rRDDtscls = (rRDDXtscl.map(lambda (id,(commentLength,body,timeSince,score,subreddit)):  \n",
    "                        (id,(commentLength,sentiment(body),timeSince,score,subreddit)))\n",
    "                      .cache()\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "rRDDtscls.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "rRDDtscls.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.0 Set up logistic regression inputs with OHE features for categorical variable subredddit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate label from score using srDigestR and create rawData RDD in proper format:  (label, non-categorical variables, categorical variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def label(score, subreddit, percentMap):\n",
    "    if score <= percentMap[subreddit][0]: return 0\n",
    "    else: return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format of rRDDtscls:  (id,(commentLength,posNegDiff,timeSince,score,subreddit)))\n",
    "\n",
    "Format of rawData is a tuple:  (label, (0,commentLength), (1,posNegDiff), (2,timeSince), subreddit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def tupToString(tup):\n",
    "    return ','.join([str(item) for item in tup])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rawData = (rRDDtscls.map(lambda (id,(commentLength,posNegDiff,timeSince,score,subreddit)):  \n",
    "                    (label(score,subreddit,srDigestR), (0,commentLength), (1,posNegDiff), (2,timeSince), subreddit))\n",
    "                    .cache()\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "type(rawData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "rawData.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "rawData.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into training, validation and test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7035 882 843 8760 8760\n"
     ]
    }
   ],
   "source": [
    "weights = [.8, .1, .1]\n",
    "seed = 42\n",
    "# Use randomSplit with weights and seed\n",
    "rawTrainData, rawValData, rawTestData = rawData.randomSplit(weights, seed)\n",
    "\n",
    "# Cache the data\n",
    "rawTrainData.cache()\n",
    "rawValData.cache()\n",
    "rawTestData.cache()\n",
    "\n",
    "nAll = rawData.count()\n",
    "nTrain = rawTrainData.count()\n",
    "nVal = rawValData.count()\n",
    "nTest = rawTestData.count()\n",
    "print nTrain, nVal, nTest, nTrain + nVal + nTest, nAll\n",
    "\n",
    "# print rawData.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create one hot encoding dictionary.\n",
    "\n",
    "I DON'T NEED ML LAB 4 createOneHotDict BECAUSE I ALREADY HAVE A LIST OF SUBREDDITS IN srDigestR.  Just need to pull keys out of this dict to create my OHEdict. NOTE that my OHEdict is already shifted by 3 to avoid collision with non-categorical variables.\n",
    "\n",
    "<B> NEED TO FIX OHEdict:  should only pull categories out of TRAINING set, NOT entire set.  Otherwise it's cheating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OHEdict = {(0, v): k+3 for (k, v) in enumerate(srDigestR)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "print OHEdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "OHEdict[(0,'politics')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create OHETrainData."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createLabeledPoint(point,numFeats):\n",
    "    label = point[0]\n",
    "    feats = point[1:]\n",
    "    sv = SparseVector(numFeats, feats)\n",
    "    # print sv\n",
    "    return LabeledPoint(label, sv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(OHEdict)+3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "createLabeledPoint((1, (0, 36), (1, -11.0), (2, 811), (25, 1)),   len(OHEdict+3  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numFeats = len(OHEdict)+3\n",
    "OHETrainData = (rawTrainData.map(lambda (label, t1, t2, t3, sr):\n",
    "                                       (label, t1, t2, t3, (OHEdict[(0,sr)], 1) ))\n",
    "                            .map(lambda point:  createLabeledPoint(point, numFeats))\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "OHETrainData.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Create OHEValData and OHETestData; skip for now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.0 Run logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fixed hyperparameters\n",
    "numIters = 50\n",
    "stepSize = 10.\n",
    "regParam = 1e-6\n",
    "regType = 'l2'\n",
    "includeIntercept = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3742.7305146647359, -0.11871780492023463, -0.10704296413708338, -0.037927471870862851, -0.021060388933347763] 9.8895531871\n"
     ]
    }
   ],
   "source": [
    "model0 = LogisticRegressionWithSGD.train(OHETrainData, iterations = numIters, step = stepSize,\n",
    "                                        regParam = regParam, regType = regType,\n",
    "                                        intercept = includeIntercept)\n",
    "sortedWeights = sorted(model0.weights)\n",
    "# print sortedWeights[:5], model0.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
