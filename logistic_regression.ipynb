{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    " \n",
    "# Path for spark source folder\n",
    "os.environ['SPARK_HOME'] = \"/usr/local/spark\"\n",
    "\n",
    "# Append pyspark to Python Path\n",
    "sys.path.append(\"/usr/local/spark/python\")\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "# Load in the testing code and check to see if your answer is correct\n",
    "# If incorrect it will report back '1 test failed' for each failed test\n",
    "# Make sure to rerun any cell you change before trying the test again\n",
    "from test_helper import Test\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "# Append afinn to Python Path and import afinn.  Used for pulling data from percentiles.\n",
    "sys.path.append(\"/usr/local/lib/python2.7/dist-packages/afinn\")\n",
    "from afinn import Afinn\n",
    "\n",
    "# Stuff for logistic regression\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "from pyspark.mllib.linalg import SparseVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sys.path.append(\"/usr/local/lib/python2.7/dist-packages\")\n",
    "from tdigest import TDigest\n",
    "from numpy.random import random\n",
    "from operator import add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.0 Read data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in data.  For now, on local filesystem.  \n",
    "\n",
    "<B>ADD CODE TO READ FROM S3.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = sqlContext.read.json(\"/home/ubuntu/RC_2007-10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150429"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('archived', 'boolean'),\n",
       " ('author', 'string'),\n",
       " ('author_flair_css_class', 'string'),\n",
       " ('author_flair_text', 'string'),\n",
       " ('body', 'string'),\n",
       " ('controversiality', 'bigint'),\n",
       " ('created_utc', 'string'),\n",
       " ('distinguished', 'string'),\n",
       " ('downs', 'bigint'),\n",
       " ('edited', 'string'),\n",
       " ('gilded', 'bigint'),\n",
       " ('id', 'string'),\n",
       " ('link_id', 'string'),\n",
       " ('name', 'string'),\n",
       " ('parent_id', 'string'),\n",
       " ('retrieved_on', 'bigint'),\n",
       " ('score', 'bigint'),\n",
       " ('score_hidden', 'boolean'),\n",
       " ('subreddit', 'string'),\n",
       " ('subreddit_id', 'string'),\n",
       " ('ups', 'bigint')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- archived: boolean (nullable = true)\n",
      " |-- author: string (nullable = true)\n",
      " |-- author_flair_css_class: string (nullable = true)\n",
      " |-- author_flair_text: string (nullable = true)\n",
      " |-- body: string (nullable = true)\n",
      " |-- controversiality: long (nullable = true)\n",
      " |-- created_utc: string (nullable = true)\n",
      " |-- distinguished: string (nullable = true)\n",
      " |-- downs: long (nullable = true)\n",
      " |-- edited: string (nullable = true)\n",
      " |-- gilded: long (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- link_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- parent_id: string (nullable = true)\n",
      " |-- retrieved_on: long (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      " |-- score_hidden: boolean (nullable = true)\n",
      " |-- subreddit: string (nullable = true)\n",
      " |-- subreddit_id: string (nullable = true)\n",
      " |-- ups: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(archived=True, author=u'bostich', author_flair_css_class=None, author_flair_text=None, body=u'test', controversiality=0, created_utc=u'1192450635', distinguished=None, downs=0, edited=u'false', gilded=0, id=u'c0299an', link_id=u't3_5yba3', name=u't1_c0299an', parent_id=u't3_5yba3', retrieved_on=1427426409, score=1, score_hidden=False, subreddit=u'reddit.com', subreddit_id=u't5_6', ups=1)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(archived=True, author=u'bostich', author_flair_css_class=None, author_flair_text=None, body=u'test', controversiality=0, created_utc=u'1192450635', distinguished=None, downs=0, edited=u'false', gilded=0, id=u'c0299an', link_id=u't3_5yba3', name=u't1_c0299an', parent_id=u't3_5yba3', retrieved_on=1427426409, score=1, score_hidden=False, subreddit=u'reddit.com', subreddit_id=u't5_6', ups=1)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Find minimum comment timestamp for each post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each post link_id, find minimum created_utc timestamp [can't trust data is ordered by time] and store in key-value pair (pair RDD) {link_id: min_created_utc},  (Plan B:  set up API to get timestamp for all posts in Reddit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "redditRDD = df.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(archived=True, author=u'bostich', author_flair_css_class=None, author_flair_text=None, body=u'test', controversiality=0, created_utc=u'1192450635', distinguished=None, downs=0, edited=u'false', gilded=0, id=u'c0299an', link_id=u't3_5yba3', name=u't1_c0299an', parent_id=u't3_5yba3', retrieved_on=1427426409, score=1, score_hidden=False, subreddit=u'reddit.com', subreddit_id=u't5_6', ups=1)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redditRDD.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<B>I'M KEEPING TOO MUCH DATA.  AT THIS STEP, AND SUBSEQUENT STEPS, THROW OUT DATA I'M NOT USING."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rRDD = df.map(lambda r: (r.id, (r.body, int(r.created_utc), r.link_id, r.parent_id, int(r.score), r.subreddit, r.subreddit_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'c0299an',\n",
       "  (u'test', 1192450635, u't3_5yba3', u't3_5yba3', 1, u'reddit.com', u't5_6'))]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rRDD.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort to find minimum timestamp created_utc for each post link_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150429"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "minTimeRDD = (rRDD.map(lambda (k, v): (v[2],v[1])) # maps to (link_id, created_utc)\n",
    "                  .reduceByKey(lambda a, b:  a if a < b else b)\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u't3_5yxsg', 1193154263)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minTimeRDD.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24370"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minTimeRDD.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Filter to include only extreme up and down votes (top 3% of subreddit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter to retain only records that are top or bottom 3% in comment score (upvotes-downvotes) of their subreddit.  Reduces dataset for all subsequent processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find 3% and 97% percentiles for each subreddit.  Use T-digest data structure for highly accurate approximate percentiles: \n",
    "http://dataorigami.net/blogs/napkin-folding/19055451-percentile-and-quantile-estimation-of-big-data-the-t-digest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def digest1(value):\n",
    "    digest = TDigest()\n",
    "    digest.update(value)\n",
    "    return digest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to get at scores for each subreddit, THEN map using digest.\n",
    "\n",
    "\n",
    "<b>REFACTOR AND MAKE THIS RUN FASTER.  I'M CALLING DIGEST1 TOO MANY TIMES.\n",
    "\n",
    "ALSO, NEED TO SPLIT INTO TRAINING, VAL AND TEST BEFORE CREATING SUBREDDIT DIGEST ON TRAINING DATA ONLY.  OTHERWISE I'M CHEATING.\n",
    "</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subredditDigestRDD = (rRDD.map(lambda (k, v):  (v[5], v[4]))\n",
    "                        .map(lambda (k, v): (k, digest1(v)))  \n",
    "                        .reduceByKey(lambda a, b:  a + b)\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(subredditDigestRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating digest dictionary takes a few minutes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subredditDigest = subredditDigestRDD.collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tdigest.tdigest.TDigest"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(subredditDigest['politics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.3696052631578945"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subredditDigest['politics'].percentile(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.797647058823536"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subredditDigest['politics'].percentile(97)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alyssa's values for politics were (-6.0, 24.0) rounded.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set limits for what data to include by percentile.  Use only data less than lowPercentile or greater than highPercentile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lowPercentile, highPercentile = 3, 97"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "srDigestR = {key : (round(subredditDigest[key].percentile(lowPercentile)), \n",
    "                    round(subredditDigest[key].percentile(highPercentile))) \n",
    "             for key in subredditDigest.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.0, 17.0)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "srDigestR['politics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(srDigestR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'eo': (1.0, 1.0), u'arxiv': (1.0, 1.0), u'zh': (1.0, 1.0), u'features': (-1.0, 3.0), u'request': (0.0, 1.0), u'it': (1.0, 1.0), u'sv': (1.0, 1.0), u'gadgets': (-0.0, 2.0), u'nsfw': (-3.0, 11.0), u'politics': (-5.0, 17.0), u'id': (1.0, 2.0), u'es': (1.0, 1.0), u'ru': (0.0, 1.0), u'netsec': (0.0, 2.0), u'ads': (0.0, 4.0), u'entertainment': (-1.0, 2.0), u'tr': (0.0, 1.0), u'sports': (-2.0, 3.0), u'freeculture': (1.0, 1.0), u'gaming': (-0.0, 5.0), u'fr': (1.0, 1.0), u'business': (-0.0, 2.0), u'reddit.com': (-3.0, 23.0), u'de': (0.0, 4.0), u'lipstick.com': (2.0, 2.0), u'ja': (0.0, 2.0), u'science': (-4.0, 20.0), u'joel': (1.0, 1.0), u'no': (1.0, 1.0), u'programming': (-4.0, 23.0), u'bugs': (1.0, 3.0), u'sl': (1.0, 1.0)}\n"
     ]
    }
   ],
   "source": [
    "print srDigestR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter records to retain only top and bottom 3% of comment scores.\n",
    "\n",
    "<b>NOTE:  NEED TO BROADCAST srDigestR WHEN I MOVE TO CLUSTER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rRDDExtreme = rRDD.filter(lambda (k,v): v[4] < srDigestR[v[5]][0] or v[4] > srDigestR[v[5]][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8760"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rRDDExtreme.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0  Calculate timeSince"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate time since post was created based on created_utc and min_created_utc from pair RDD.  In Alyssa's IPython notebook this is called timeSince.  In her R code it's called recency.  \n",
    "\n",
    "For this I need a left outer join.  For each element (k, v) in self, the resulting RDD will either contain all pairs (k, (v, w)) for w in other, or the pair (k, (v, None)) if no elements in other have key k.  I need to know if there are records in my comment dataset for which there is no \"minimum time\", as this would indicate a processing error.\n",
    "\n",
    "Map RDD to get post link_id as key, then join with minTimeRDD.\n",
    "\n",
    "<b>CAN I USE DATAFRAMES FOR ALL THIS? GETTING TIRED OF REMEMBERING WHAT v[4] IS.  </b>\n",
    "\n",
    "The only data I need for regression are:  C(subreddit) + timeSince + commentLength + posNegDiff.  Need to keep only comment id (key), score, subreddit, timeSince, comment text from this step.\n",
    "\n",
    "Format of output RDD is (id,(body,timeSince,score,subreddit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rRDDXts = (rRDDExtreme.map(lambda (k,v):  (v[2],(k,v[0],v[1],v[2],v[3],v[4],v[5],v[6])))  # pull link_id as key\n",
    "                      .leftOuterJoin(minTimeRDD) # join on link_id (post)\n",
    "                      .map(lambda (link_id,(x,min_utc)):  (x[0], (x[1],x[2]-min_utc,x[5],x[6])))\n",
    "                      .cache()\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'c02biau', (u'[deleted]', 0, -10, u'reddit.com')),\n",
       " (u'c029tih',\n",
       "  (u\"Basically, the hospital's position amounts to:\\n\\n&gt; If she can't hold her roofies she deserves to be assfucked and denied medical care and collection of evidence!\\n\\nNot the *most* progressive attitude...\",\n",
       "   0,\n",
       "   70,\n",
       "   u'reddit.com')),\n",
       " (u'c029tkl',\n",
       "  (u'I guess I kind of understand how a drunk person cannot \"consent\" to a rape kit because of the type of tests performed. But jesus, that\\'s when most rape cases happen -- especially date rape! Sad.',\n",
       "   811,\n",
       "   36,\n",
       "   u'reddit.com')),\n",
       " (u'c029tl9',\n",
       "  (u'\"an aftereffect of date rape drug?\" No.\\r\\n\\r\\nThe Reddit headline is not the place for original research.',\n",
       "   1151,\n",
       "   -26,\n",
       "   u'reddit.com')),\n",
       " (u'c029tmd',\n",
       "  (u\"What the hell are you trying to say? Sure the 'after' was an unnecessary addition to the 'effect', as intoxication is actually the *primary* effect of date rape drugs, but a minor semantics error isn't worth a vitriolic comment.\",\n",
       "   1705,\n",
       "   27,\n",
       "   u'reddit.com')),\n",
       " (u'c029ts6',\n",
       "  (u' A lot of this story doesn\\'t make sense.  I was under the impression that if you get raped you call 9-1-1 afterwards and talk to police and then get a rape kit.  What is with going to the police only after being denied a rape kit twice?\\n\\nI\\'m not trying to be critical, I\\'m just really not clear on the order of operations of post-rape treatment.  Particularly if she had two friends with her, why did nobody call the police?  Why didn\\'t they go to another hospital?  What kind of friends are those?  \"Don\\'t call the police, just go get some sleep and we\\'ll go to the hospital again in the morning.\"',\n",
       "   4964,\n",
       "   -9,\n",
       "   u'reddit.com')),\n",
       " (u'c029ttc',\n",
       "  (u\"If I were a guessing type of person, I would say that she felt so ashamed when she was rejected by the clinic/hospital/whatever that she was probably mentally unable to go to the police.\\n\\nNow as to why she didn't go there in the first place, I have no idea. Proximity? Easy access? Dunno.\\n\\nAlso as you said, were where her friends?\\n\\nEither way, the school totally failed on this one and I hope they learn their lesson.\",\n",
       "   5959,\n",
       "   24,\n",
       "   u'reddit.com')),\n",
       " (u'c029u1x',\n",
       "  (u\"But you don't have to go to police, they come to you.  Just call 9-1-1.  I mean, this should have happened before she ever went to the hospital in the first place.\\n\\nIt says she went to the hospital with her friends at first.  How can you, as a friend (or even as an unfamiliar third party), NOT call 9-1-1 after you find out a rape just occurred?  It makes absolutely no sense at all to me.\\n\\nClearly proximity and easy access weren't the problem.  This is downtown DC we're talking about.  That place is crawling with cops.\",\n",
       "   11762,\n",
       "   -5,\n",
       "   u'reddit.com')),\n",
       " (u'c029u5i',\n",
       "  (u\"I suspect that shock was a big factor, and maybe the victim didn't want to tell the police.\\n\\nIt's understandable for people to act a little screwy in that situation. I've heard of people who have been raped and then gone and done a full day of work before telling anyone. I'm not sure you can predict how people will behave\",\n",
       "   13873,\n",
       "   24,\n",
       "   u'reddit.com')),\n",
       " (u'c029u7f',\n",
       "  (u\"If you don't want to tell police, what's the point of getting a rape kit?\\n\\nI mean, you're begging to have a stranger stick a cotton swab in you ass, but you're not willing to write a few paragraphs on a sheet of carbon paper?\\n\\nI completely understand that she might behave unusually, but she was with friends.  Was there no rational person in the bunch?\",\n",
       "   15666,\n",
       "   -8,\n",
       "   u'reddit.com'))]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rRDDXts.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.0  Calculate commentLength"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean comment body and calculate commentLength.\n",
    "\n",
    "R gsub:\n",
    "gsub(pattern, replacement, x, ignore.case = FALSE, perl = FALSE, fixed = FALSE, useBytes = FALSE)\n",
    "\n",
    "Python re:\n",
    "re.sub(pattern, repl, string, count=0, flags=0).  Return the string obtained by replacing the leftmost non-overlapping occurrences of pattern in string by the replacement repl.\n",
    "\n",
    "<B>NOTE:  ALYSSA REMOVED QUOTED COMMENTS.  I REMOVING THEM ALSO BUT IN MY FIRST EXAMPLE I FOUND A \"MADE UP\" QUOTE THAT ISN'T REALLY QUOTING SOMEONE ELSE'S POST.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanup(body):\n",
    "\n",
    "\t# Recode HTML codes\n",
    "\tbody = re.sub(\"&gt;\", \">\", body)\n",
    "\tbody = re.sub(\"&lt;\", \"<\", body)\n",
    "\tbody = re.sub(\"&amp;\", \"&\", body)\n",
    "\tbody = re.sub(\"&nbsp;\", \" \", body)\n",
    "\n",
    "\t# Remove deleted\n",
    "\tbody = re.sub(\"^[deleted]$\", \"\", body)\n",
    "\n",
    "\t# Remove URL\n",
    "\tbody = re.sub(\"http[[:alnum:][:punct:]]*\", \" \", body) # url\n",
    "\n",
    "\t# Remove /r/subreddit, /u/user\n",
    "\tbody = re.sub(\"/r/[[:alnum:]]+|/u/[[:alnum:]]+\", \" \", body)\n",
    "\n",
    "\t# Remove quoted comments\n",
    "\tbody = re.sub(\"(>.*?\\\\n\\\\n)+\", \" \", body)\n",
    "\n",
    "\t# Remove control characters (\\n, \\b)\n",
    "\tbody = re.sub(\"[[:cntrl:]]\", \" \", body)\n",
    "\n",
    "\t# Remove single quotation marks (contractions)\n",
    "\tbody = re.sub(\"'\", \"\", body)\n",
    "\n",
    "\t# Remove punctuation\n",
    "\tbody = re.sub(\"[[:punct:]]\", \" \", body)\n",
    "\n",
    "\t# Replace multiple spaces with single space\n",
    "\tbody = re.sub(\"\\\\s+\", \" \", body) # Multiple spaces\n",
    "\t# body = re.sub(\"^\\\\s+\", \"\", body) # Space at the start of the string\n",
    "\t# body = re.sub(\"+\\\\s$\", \"\", body) # Space at the end of the string\n",
    "\tbody = body.strip()\n",
    "\n",
    "\t# Lower case\n",
    "\tbody = body.lower()\n",
    "\n",
    "\t# Return comment length (number of words) and body (cleaned up text)\n",
    "\treturn body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clbody = cleanup(u\"Basically, the hospital's position amounts to:\\n\\n&gt; If she can't hold her roofies she deserves to be assfucked and denied medical care and collection of evidence!\\n\\nNot the *most* progressive attitude...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "print len(clbody.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basically, the hospitals position amounts to: not the *most* progressive attitude...\n"
     ]
    }
   ],
   "source": [
    "print clbody"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current format of RDD:  (id,(body,timeSince,score,subreddit))\n",
    "Format of rRDDXtscl:  (id,(commentLength,body,timeSince,score,subreddit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rRDDXtscl = (rRDDXts.map(lambda (id,(body,timeSince,score,subreddit)): (id,(cleanup(body),timeSince,score,subreddit)))\n",
    "                    .map(lambda (id,(body,timeSince,score,subreddit)): (id,(len(body.split()),body,timeSince,score,subreddit)))\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.0 (Filter out exclusions if necessary; skip for now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out exclusions.  Further reduces dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.0 Run sentiment analysis and calculate posNegDiff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use AFINN model to do sentiment analysis.\n",
    "\n",
    "Finn Årup Nielsen, \"A new ANEW: evaluation of a word list for sentiment analysis in microblogs\" , Proceedings of the ESWC2011 Workshop on 'Making Sense of Microposts': Big things come in small packages 718 in CEUR Workshop Proceedings: 93-98. 2011 May. Matthew Rowe, Milan Stankovic, Aba-Sah Dadzie, Mariann Hardey (editors)\n",
    "\n",
    "<B>I'M WONDERING IF CREATING AN AFINN OBJECT EACH TIME IS TAKING TOO LONG.  MIGHT WANT TO REFACTOR THIS TO A PYTHON FUNCTION WITH A DICT LOOKUP AND USE A BROADCAST VARIABLE.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentiment(body):\n",
    "    afinn = Afinn()\n",
    "    return afinn.score(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment(\"This is utterly excellent!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rRDDtscls = (rRDDXtscl.map(lambda (id,(commentLength,body,timeSince,score,subreddit)):  \n",
    "                        (id,(commentLength,sentiment(body),timeSince,score,subreddit)))\n",
    "                      .cache()\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'c02biau', (1, 0.0, 0, -10, u'reddit.com')),\n",
       " (u'c029tih', (11, 0.0, 0, 70, u'reddit.com')),\n",
       " (u'c029tkl', (36, -11.0, 811, 36, u'reddit.com')),\n",
       " (u'c029tl9', (17, -5.0, 1151, -26, u'reddit.com')),\n",
       " (u'c029tmd', (39, -11.0, 1705, 27, u'reddit.com'))]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rRDDtscls.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8760"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rRDDtscls.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.0 Set up logistic regression inputs with OHE features for categorical variable subredddit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate label from score using srDigestR and create rawData RDD in proper format:  (label, non-categorical variables, categorical variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def label(score, subreddit, percentMap):\n",
    "    if score <= percentMap[subreddit][0]: return 0\n",
    "    else: return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format of rRDDtscls:  (id,(commentLength,posNegDiff,timeSince,score,subreddit)))\n",
    "\n",
    "Format of rawData is a tuple:  (label, (0,commentLength), (1,posNegDiff), (2,timeSince), subreddit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def tupToString(tup):\n",
    "    return ','.join([str(item) for item in tup])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rawData = (rRDDtscls.map(lambda (id,(commentLength,posNegDiff,timeSince,score,subreddit)):  \n",
    "                    (label(score,subreddit,srDigestR), (0,commentLength), (1,posNegDiff), (2,timeSince), subreddit))\n",
    "                    .cache()\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rawData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, (0, 1), (1, 0.0), (2, 0), u'reddit.com'),\n",
       " (1, (0, 11), (1, 0.0), (2, 0), u'reddit.com'),\n",
       " (1, (0, 36), (1, -11.0), (2, 811), u'reddit.com'),\n",
       " (0, (0, 17), (1, -5.0), (2, 1151), u'reddit.com'),\n",
       " (1, (0, 39), (1, -11.0), (2, 1705), u'reddit.com')]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawData.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8760"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawData.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into training, validation and test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7035 882 843 8760 8760\n"
     ]
    }
   ],
   "source": [
    "weights = [.8, .1, .1]\n",
    "seed = 42\n",
    "# Use randomSplit with weights and seed\n",
    "rawTrainData, rawValData, rawTestData = rawData.randomSplit(weights, seed)\n",
    "\n",
    "# Cache the data\n",
    "rawTrainData.cache()\n",
    "rawValData.cache()\n",
    "rawTestData.cache()\n",
    "\n",
    "nAll = rawData.count()\n",
    "nTrain = rawTrainData.count()\n",
    "nVal = rawValData.count()\n",
    "nTest = rawTestData.count()\n",
    "print nTrain, nVal, nTest, nTrain + nVal + nTest, nAll\n",
    "\n",
    "# print rawData.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create one hot encoding dictionary.\n",
    "\n",
    "I DON'T NEED ML LAB 4 createOneHotDict BECAUSE I ALREADY HAVE A LIST OF SUBREDDITS IN srDigestR.  Just need to pull keys out of this dict to create my OHEdict. NOTE that my OHEdict is already shifted by 3 to avoid collision with non-categorical variables.\n",
    "\n",
    "<B> NEED TO FIX OHEdict:  should only pull categories out of TRAINING set, NOT entire set.  Otherwise it's cheating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OHEdict = {(0, v): k+3 for (k, v) in enumerate(srDigestR)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(0, u'sports'): 20, (0, u'nsfw'): 11, (0, u'sv'): 9, (0, u'gadgets'): 10, (0, u'no'): 31, (0, u'science'): 29, (0, u'programming'): 32, (0, u'gaming'): 22, (0, u'tr'): 19, (0, u'business'): 24, (0, u'ads'): 17, (0, u'joel'): 30, (0, u'zh'): 5, (0, u'id'): 13, (0, u'fr'): 23, (0, u'freeculture'): 21, (0, u'ru'): 15, (0, u'lipstick.com'): 27, (0, u'politics'): 12, (0, u'bugs'): 33, (0, u'reddit.com'): 25, (0, u'arxiv'): 4, (0, u'features'): 6, (0, u'es'): 14, (0, u'netsec'): 16, (0, u'it'): 8, (0, u'entertainment'): 18, (0, u'request'): 7, (0, u'de'): 26, (0, u'ja'): 28, (0, u'eo'): 3, (0, u'sl'): 34}\n"
     ]
    }
   ],
   "source": [
    "print OHEdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OHEdict[(0,'politics')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create OHETrainData."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createLabeledPoint(point,numFeats):\n",
    "    label = point[0]\n",
    "    feats = point[1:]\n",
    "    sv = SparseVector(numFeats, feats)\n",
    "    # print sv\n",
    "    return LabeledPoint(label, sv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(OHEdict)+3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "createLabeledPoint((1, (0, 36), (1, -11.0), (2, 811), (25, 1)),   len(OHEdict+3  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numFeats = len(OHEdict)+3\n",
    "OHETrainData = (rawTrainData.map(lambda (label, t1, t2, t3, sr):\n",
    "                                       (label, t1, t2, t3, (OHEdict[(0,sr)], 1) ))\n",
    "                            .map(lambda point:  createLabeledPoint(point, numFeats))\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(0.0, (35,[0,1,2,25],[1.0,0.0,0.0,1.0])),\n",
       " LabeledPoint(1.0, (35,[0,1,2,25],[11.0,0.0,0.0,1.0])),\n",
       " LabeledPoint(1.0, (35,[0,1,2,25],[36.0,-11.0,811.0,1.0])),\n",
       " LabeledPoint(0.0, (35,[0,1,2,25],[17.0,-5.0,1151.0,1.0])),\n",
       " LabeledPoint(1.0, (35,[0,1,2,25],[39.0,-11.0,1705.0,1.0])),\n",
       " LabeledPoint(0.0, (35,[0,1,2,25],[112.0,-11.0,4964.0,1.0])),\n",
       " LabeledPoint(1.0, (35,[0,1,2,25],[78.0,-3.0,5959.0,1.0])),\n",
       " LabeledPoint(0.0, (35,[0,1,2,25],[98.0,-5.0,11762.0,1.0])),\n",
       " LabeledPoint(0.0, (35,[0,1,2,25],[66.0,-8.0,15666.0,1.0])),\n",
       " LabeledPoint(0.0, (35,[0,1,2,25],[9.0,0.0,17470.0,1.0]))]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OHETrainData.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Create OHEValData and OHETestData; skip for now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.0 Run logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fixed hyperparameters\n",
    "numIters = 50\n",
    "stepSize = 10.\n",
    "regParam = 1e-6\n",
    "regType = 'l2'\n",
    "includeIntercept = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3742.7305146647359, -0.11871780492023463, -0.10704296413708338, -0.037927471870862851, -0.021060388933347763] 9.8895531871\n"
     ]
    }
   ],
   "source": [
    "model0 = LogisticRegressionWithSGD.train(OHETrainData, iterations = numIters, step = stepSize,\n",
    "                                        regParam = regParam, regType = regType,\n",
    "                                        intercept = includeIntercept)\n",
    "sortedWeights = sorted(model0.weights)\n",
    "print sortedWeights[:5], model0.intercept"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
