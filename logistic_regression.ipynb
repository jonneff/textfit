{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "# Force matplotlib to not use any Xwindows backend.\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    " \n",
    "# Path for spark source folder\n",
    "os.environ['SPARK_HOME'] = \"/usr/local/spark\"\n",
    "\n",
    "# Append pyspark to Python Path\n",
    "sys.path.append(\"/usr/local/spark/python\")\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "# Load in the testing code and check to see if your answer is correct\n",
    "# If incorrect it will report back '1 test failed' for each failed test\n",
    "# Make sure to rerun any cell you change before trying the test again\n",
    "from test_helper import Test\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import HiveContext\n",
    "# sc = SparkContext() # not needed in IPython notebook.\n",
    "sqlContext = SQLContext(sc)\n",
    "hiveContext = HiveContext(sc)\n",
    "\n",
    "# Append afinn to Python Path and import afinn.  Used for pulling data from percentiles.\n",
    "sys.path.append(\"/usr/local/lib/python2.7/dist-packages/afinn\")\n",
    "from afinn import Afinn\n",
    "\n",
    "# Stuff for logistic regression\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "from pyspark.sql.types import StructField, BooleanType, StringType, LongType, StructType\n",
    "sys.path.append(\"/usr/local/lib/python2.7/dist-packages\")\n",
    "from tdigest import TDigest\n",
    "from numpy.random import random\n",
    "from operator import add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Read data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read using json schema.  If you don't use schema on read, Spark reads ENTIRE FILE to infer schema BEFORE actually reading in data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define json schema to speed up reading json files in S3\n",
    "\n",
    "fields = [StructField(\"archived\", BooleanType(), True),\n",
    "        StructField(\"author\", StringType(), True),\n",
    "        StructField(\"author_flair_css_class\", StringType(), True),\n",
    "        StructField(\"body\", StringType(), True),\n",
    "        StructField(\"controversiality\", LongType(), True),\n",
    "        StructField(\"created_utc\", StringType(), True),\n",
    "        StructField(\"distinguished\", StringType(), True),\n",
    "        StructField(\"downs\", LongType(), True),\n",
    "        StructField(\"edited\", StringType(), True),\n",
    "        StructField(\"gilded\", LongType(), True),\n",
    "        StructField(\"id\", StringType(), True),\n",
    "        StructField(\"link_id\", StringType(), True),\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"parent_id\", StringType(), True),\n",
    "        StructField(\"retrieved_on\", LongType(), True),\n",
    "        StructField(\"score\", LongType(), True),\n",
    "        StructField(\"score_hidden\", BooleanType(), True),\n",
    "        StructField(\"subreddit\", StringType(), True),\n",
    "        StructField(\"subreddit_id\", StringType(), True),\n",
    "        StructField(\"ups\", LongType(), True)]\n",
    "df = hiveContext.read.json(\"s3n://reddit-comments/2012/*\", StructType(fields))\n",
    "# df.take(5)\n",
    "\n",
    "# Refactor later to filter based on subreddits list. You can't do \"in list\" with SQL but maybe dataframe DSL. \n",
    "# Filter down to subreddits of interest\n",
    "subreddits = [u'leagueoflegends', u'GirlGamers', u'pics', u'politics']\n",
    "df2 = (df.filter(  (df.subreddit == u'leagueoflegends') \n",
    "                 | (df.subreddit == u'GirlGamers')\n",
    "                 | (df.subreddit == u'pics')\n",
    "                 | (df.subreddit == u'politics') )\n",
    "           # .coalesce(400) Can't coalesce here with 1 TB input.  If you do, it coalesces prematurely and read fails.\n",
    "           .persist(StorageLevel.MEMORY_AND_DISK_SER)\n",
    "           )\n",
    "df2.count() # Forces read in df, execute df2 BEFORE coalesce().\n",
    "\n",
    "# df2.coalesce(400)  # Now coalesce after read.  THIS DOESN'T WORK.  IGNORES coalesce()\n",
    "\n",
    "isInData = {key: True for key in subreddits}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Comment out check for records if you know the subreddits are there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "def is_there(df,srList):\n",
    "    return {key : True if df2.filter((df.subreddit == key)).take(1) else False for key in srList }\n",
    "\n",
    "isInData = is_there(df2, subreddits)\n",
    "print isInData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>DO NOT CUT MAY NEED THIS LATER:  code below finds out how many records there are for each subreddit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "subreddits = [u'leagueoflegends', u'GirlGamers', u'pics', u'politics']\n",
    "\n",
    "def create_counts(df,srList):\n",
    "    return {key : df2.filter((df.subreddit == key)).count() for key in srList}\n",
    "\n",
    "srCounts = create_counts(df2, subreddits)\n",
    "print srCounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Filter to include only extreme up and down votes (top 3% of subreddit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter to retain only records that are top or bottom 3% in comment score (upvotes-downvotes) of their subreddit.  Reduces dataset for all subsequent processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ALTERNATIVE calculation of 3 and 97 percentiles using SQL and HiveQL percentile estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'politics': (-4.0, 24.0), u'GirlGamers': (-3.0, 20.0), u'leagueoflegends': (-4.0, 27.0), u'pics': (-3.0, 42.0)}\n"
     ]
    }
   ],
   "source": [
    "def createsrDict(df, srList, isIn):\n",
    "    hiveContext.registerDataFrameAsTable(df, \"rcomments\")\n",
    "    srDigest = {}\n",
    "    for key in srList:\n",
    "        if isIn[key]:  # if the subreddit is in the input data set \n",
    "            # not sure if percentile() [integers] or percentile_approx() [double] runs faster.\n",
    "            SQL = \"select percentile(score, array(0.03,0.97)) from rcomments where subreddit==\"+\"'\" + key + \"'\"\n",
    "            srDigest[key] = hiveContext.sql(SQL).collect()[0][0]\n",
    "    return srDigest\n",
    "\n",
    "subredditDigest = createsrDict(df2, subreddits, isInData) \n",
    "\n",
    "srDigestR = {key : (round(subredditDigest[key][0]), \n",
    "                    round(subredditDigest[key][1]) ) \n",
    "             for key in subredditDigest.keys()}\n",
    "print srDigestR  \n",
    "\n",
    "# Put Dataframe into vanilla RDD\n",
    "\n",
    "rRDD = (df2.map(lambda r: (r.id, (r.body, int(r.created_utc), r.link_id, r.parent_id, int(r.score), r.subreddit, r.subreddit_id)))\n",
    "          # .setName(\"rRDD\")\n",
    "          # .persist(StorageLevel.MEMORY_AND_DISK_SER) # Do not persist.  Not used after rRDDExtreme.\n",
    "       )\n",
    "# rRDD.take(5)\n",
    "\n",
    "rRDDExtreme = (rRDD.filter(lambda (k,v): v[4] < srDigestR[v[5]][0] or v[4] > srDigestR[v[5]][1])\n",
    "                  .coalesce(400)\n",
    "                  .setName(\"rRDDExtreme\")\n",
    "                  .persist(StorageLevel.MEMORY_AND_DISK_SER)\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alyssa's values for politics were (-6.0, 24.0) rounded for April and May 2015.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "print srDigestR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Find minimum comment timestamp for each post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn rRDDExtreme into dataframe of just link_id.   Format of rRDD and rRDDExtreme:  \n",
    "(r.id, (r.body, int(r.created_utc), r.link_id, r.parent_id, int(r.score), r.subreddit, r.subreddit_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find min time comments for 100% of subreddits of interest using HiveQL.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>LATER I CAN READ REAL POST created_utc FROM CSV FILES ON MASTER /mnt/my-data/reddit_posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>COALESCING minTimeDF MIGHT IMPROVE EXECUTION."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rRDDExtremeLinks = rRDDExtreme.map(lambda (k,v): [v[2]])\n",
    "dfExtreme = sqlContext.createDataFrame(rRDDExtremeLinks,[\"xlink_id\"]).distinct()\n",
    "# sqlContext.registerDataFrameAsTable(dfExtreme, \"xtable\")\n",
    "\n",
    "# Find minimum time comments for each post and register as table\n",
    "\n",
    "minTimeDF = hiveContext.sql(\"select link_id, min(cast (created_utc as int)) as min_utc from rcomments group by link_id\")\n",
    "    \n",
    "# sqlContext.registerDataFrameAsTable(minTimeDF, \"mintable\")\n",
    "\n",
    "# Create new dataframe with min time utc for ONLY link_id's referenced in top/bottom 3%.\n",
    "\n",
    "minTimeDFX = dfExtreme.join(minTimeDF, dfExtreme.xlink_id == minTimeDF.link_id, 'inner').drop('xlink_id')\n",
    "# SQL = \"select link_id, min_utc from mintable inner join xtable on mintable.link_id = xtable.link_id\"\n",
    "# minTimeDFX = sqlContext.sql(SQL)\n",
    "minTimeDict = dict(minTimeDFX.collect())\n",
    "minTimeBR = sc.broadcast(minTimeDict)\n",
    "# len(minTimeDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80367"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(minTimeDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "minTimeDFX.columns are ['link_id', 'min_utc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example row of minTimeDFX:  Row(link_id=u't3_31j8f1', link_id=u't3_31j8f1', min_utc=1428253005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0  Calculate timeSince"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate time since post was created based on created_utc and min_created_utc from pair RDD.  In Alyssa's IPython notebook this is called timeSince.  In her R code it's called recency.  \n",
    "\n",
    "Using broadcast variable avoids  having to do a join.  \n",
    "\n",
    "Map RDD to get post link_id as key, then subtract minTime to get timeSince.\n",
    "\n",
    "Format of output RDD is (id,(body,timeSince,score,subreddit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate timeSince\n",
    "\n",
    "rRDDXts = (rRDDExtreme.map(lambda (k,v):  (v[2],(k,v[0],v[1],v[2],v[3],v[4],v[5],v[6])))  # pull link_id as key\n",
    "                      .map(lambda (link_id,(x)):  (x[0], (x[1],x[2]-minTimeBR.value[link_id],x[5],x[6])))\n",
    "                      .setName(\"rRDDXts\")\n",
    "                      .persist(StorageLevel.MEMORY_AND_DISK_SER)\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.0  Calculate commentLength"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean comment body and calculate commentLength.\n",
    "\n",
    "R gsub:\n",
    "gsub(pattern, replacement, x, ignore.case = FALSE, perl = FALSE, fixed = FALSE, useBytes = FALSE)\n",
    "\n",
    "Python re:\n",
    "re.sub(pattern, repl, string, count=0, flags=0).  Return the string obtained by replacing the leftmost non-overlapping occurrences of pattern in string by the replacement repl.\n",
    "\n",
    "<B>NOTE:  ALYSSA REMOVED QUOTED COMMENTS.  I REMOVING THEM ALSO BUT IN MY FIRST EXAMPLE I FOUND A \"MADE UP\" QUOTE THAT ISN'T REALLY QUOTING SOMEONE ELSE'S POST.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanup(body):\n",
    "\n",
    "\t# Recode HTML codes\n",
    "\tbody = re.sub(\"&gt;\", \">\", body)\n",
    "\tbody = re.sub(\"&lt;\", \"<\", body)\n",
    "\tbody = re.sub(\"&amp;\", \"&\", body)\n",
    "\tbody = re.sub(\"&nbsp;\", \" \", body)\n",
    "\n",
    "\t# Remove deleted\n",
    "\tbody = re.sub(\"^[deleted]$\", \"\", body)\n",
    "\n",
    "\t# Remove URL\n",
    "\tbody = re.sub(\"http[[:alnum:][:punct:]]*\", \" \", body) # url\n",
    "\n",
    "\t# Remove /r/subreddit, /u/user\n",
    "\tbody = re.sub(\"/r/[[:alnum:]]+|/u/[[:alnum:]]+\", \" \", body)\n",
    "\n",
    "\t# Remove quoted comments\n",
    "\tbody = re.sub(\"(>.*?\\\\n\\\\n)+\", \" \", body)\n",
    "\n",
    "\t# Remove control characters (\\n, \\b)\n",
    "\tbody = re.sub(\"[[:cntrl:]]\", \" \", body)\n",
    "\n",
    "\t# Remove single quotation marks (contractions)\n",
    "\tbody = re.sub(\"'\", \"\", body)\n",
    "\n",
    "\t# Remove punctuation\n",
    "\tbody = re.sub(\"[[:punct:]]\", \" \", body)\n",
    "\n",
    "\t# Replace multiple spaces with single space\n",
    "\tbody = re.sub(\"\\\\s+\", \" \", body) # Multiple spaces\n",
    "\t# body = re.sub(\"^\\\\s+\", \"\", body) # Space at the start of the string\n",
    "\t# body = re.sub(\"+\\\\s$\", \"\", body) # Space at the end of the string\n",
    "\tbody = body.strip()\n",
    "\n",
    "\t# Lower case\n",
    "\tbody = body.lower()\n",
    "\n",
    "\t# Return comment length (number of words) and body (cleaned up text)\n",
    "\treturn body\n",
    "\n",
    "rRDDXtscl = (rRDDXts.map(lambda (id,(body,timeSince,score,subreddit)): (id,(cleanup(body),timeSince,score,subreddit)))\n",
    "                    .map(lambda (id,(body,timeSince,score,subreddit)): (id,(len(body.split()),body,timeSince,score,subreddit)))\n",
    "             ).setName(\"rRDDXtscl\").persist(StorageLevel.MEMORY_AND_DISK_SER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "clbody = cleanup(u\"Basically, the hospital's position amounts to:\\n\\n&gt; If she can't hold her roofies she deserves to be a**f****d and denied medical care and collection of evidence!\\n\\nNot the *most* progressive attitude...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current format of RDD:  (id,(body,timeSince,score,subreddit))\n",
    "Format of rRDDXtscl:  (id,(commentLength,body,timeSince,score,subreddit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.0 (Filter out exclusions if necessary; skip for now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out exclusions.  Further reduces dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>NOTE:  THIS MIGHT BE RESPONSIBLE FOR POOR ACCURACY; I'M NOT GETTING RID OF OUTLIERS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.0 Run sentiment analysis and calculate posNegDiff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use AFINN model to do sentiment analysis.\n",
    "\n",
    "Finn Ã…rup Nielsen, \"A new ANEW: evaluation of a word list for sentiment analysis in microblogs\" , Proceedings of the ESWC2011 Workshop on 'Making Sense of Microposts': Big things come in small packages 718 in CEUR Workshop Proceedings: 93-98. 2011 May. Matthew Rowe, Milan Stankovic, Aba-Sah Dadzie, Mariann Hardey (editors)\n",
    "\n",
    "<B>I'M WONDERING IF CREATING AN AFINN OBJECT EACH TIME IS TAKING TOO LONG.  MIGHT WANT TO REFACTOR THIS TO A PYTHON FUNCTION WITH A DICT LOOKUP AND USE A BROADCAST VARIABLE.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentiment(body):\n",
    "    afinn = Afinn()\n",
    "    return afinn.score(body)\n",
    "\n",
    "rRDDtscls = (rRDDXtscl.map(lambda (id,(commentLength,body,timeSince,score,subreddit)):  \n",
    "                        (id,(commentLength,sentiment(body),timeSince,score,subreddit)))\n",
    "                      .setName(\"rRDDtscls\")\n",
    "                      .persist(StorageLevel.MEMORY_AND_DISK_SER)\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.0 Set up logistic regression inputs with OHE features for categorical variable subredddit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate label from score using srDigestR and create rawData RDD in proper format:  (label, non-categorical variables, categorical variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format of rRDDtscls:  (id,(commentLength,posNegDiff,timeSince,score,subreddit)))\n",
    "\n",
    "Format of rawData is a tuple:  (label, (0,commentLength), (1,posNegDiff), (2,timeSince), subreddit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def label(score, subreddit, percentMap):\n",
    "    if score <= percentMap[subreddit][0]: return 0\n",
    "    else: return 1\n",
    "    \n",
    "rawData = (rRDDtscls.map(lambda (id,(commentLength,posNegDiff,timeSince,score,subreddit)):  \n",
    "                    (label(score,subreddit,srDigestR), (0,commentLength), (1,posNegDiff), (2,timeSince), subreddit))\n",
    "                    .setName(\"rawData\")\n",
    "                    .persist(StorageLevel.MEMORY_AND_DISK_SER)\n",
    "          )    \n",
    "\n",
    "weights = [.8, .1, .1]\n",
    "seed = 42\n",
    "# Use randomSplit with weights and seed\n",
    "rawTrainData, rawValData, rawTestData = rawData.randomSplit(weights, seed)\n",
    "\n",
    "# Cache the data\n",
    "rawTrainData.setName(\"rawTrainData\").persist(StorageLevel.MEMORY_AND_DISK_SER)\n",
    "rawValData.setName(\"rawValData\").persist(StorageLevel.MEMORY_AND_DISK_SER)\n",
    "rawTestData.setName(\"rawTestData\").persist(StorageLevel.MEMORY_AND_DISK_SER)\n",
    "\n",
    "# These counts are expensive:  ~1 hour for 10 GB input data.\n",
    "# nAll = rawData.count()\n",
    "# nTrain = rawTrainData.count()\n",
    "# nVal = rawValData.count()\n",
    "# nTest = rawTestData.count()\n",
    "# print nTrain, nVal, nTest, nTrain + nVal + nTest, nAll\n",
    "\n",
    "# print rawData.take(1)\n",
    "\n",
    "# Create one hot encoding mapping, format LabeledPoint, and set up training data.\n",
    "\n",
    "def createOHEMap(sr):\n",
    "    if sr == u'leagueoflegends': return (3,1)\n",
    "    elif sr == u'pics' : return (4,1)\n",
    "    elif sr == u'politics' : return (5,1)\n",
    "    else: return (5,0)\n",
    "    \n",
    "def createLabeledPoint(point,numFeats):\n",
    "    label = point[0]\n",
    "    feats = point[1:]\n",
    "    sv = SparseVector(numFeats, feats)\n",
    "    # print sv\n",
    "    return LabeledPoint(label, sv)\n",
    "\n",
    "# numFeats = len(OHEdict)+3\n",
    "numFeats = 6\n",
    "OHETrainData = (rawTrainData.map(lambda (label, t1, t2, t3, sr):\n",
    "                                       (label, t1, t2, t3, createOHEMap(sr) ))\n",
    "                            .map(lambda point:  createLabeledPoint(point, numFeats))\n",
    "                            .setName(\"OHETrainData\")\n",
    "                            .persist(StorageLevel.MEMORY_AND_DISK_SER)\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(0.0, (6,[0,1,2,4],[1.0,0.0,0.0,1.0])),\n",
       " LabeledPoint(0.0, (6,[0,1,2,5],[13.0,0.0,0.0,1.0])),\n",
       " LabeledPoint(0.0, (6,[0,1,2,5],[113.0,0.0,8.0,1.0])),\n",
       " LabeledPoint(1.0, (6,[0,1,2,4],[14.0,2.0,44.0,1.0])),\n",
       " LabeledPoint(1.0, (6,[0,1,2,3],[19.0,0.0,0.0,1.0])),\n",
       " LabeledPoint(1.0, (6,[0,1,2,4],[9.0,0.0,109.0,1.0])),\n",
       " LabeledPoint(1.0, (6,[0,1,2,5],[90.0,-7.0,158.0,1.0])),\n",
       " LabeledPoint(0.0, (6,[0,1,2,4],[7.0,-1.0,161.0,1.0])),\n",
       " LabeledPoint(0.0, (6,[0,1,2,5],[17.0,0.0,90.0,1.0])),\n",
       " LabeledPoint(1.0, (6,[0,1,2,4],[6.0,-2.0,355.0,1.0]))]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OHETrainData.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Create OHEValData and OHETestData; skip for now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "OHEValData = (rawValData.map(lambda (label, t1, t2, t3, sr):\n",
    "                                       (label, t1, t2, t3, (OHEdict[(0,sr)], 1) ))\n",
    "                            .map(lambda point:  createLabeledPoint(point, numFeats))\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "OHETestData = (rawTestData.map(lambda (label, t1, t2, t3, sr):\n",
    "                                       (label, t1, t2, t3, (OHEdict[(0,sr)], 1) ))\n",
    "                            .map(lambda point:  createLabeledPoint(point, numFeats))\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.0 Run logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fixed hyperparameters\n",
    "numIters = 50\n",
    "stepSize = 1.\n",
    "regParam = 1e-6\n",
    "regType = 'l2'\n",
    "includeIntercept = True\n",
    "validateData = False\n",
    "\n",
    "model0 = LogisticRegressionWithSGD.train(OHETrainData, iterations = numIters, step = stepSize,\n",
    "                                        regParam = regParam, regType = regType,\n",
    "                                        intercept = includeIntercept, validateData = validateData)\n",
    "# sortedWeights = sorted(model0.weights)\n",
    "# print sortedWeights[:5], model0.intercept\n",
    "print model0.weights, model0.intercept\n",
    "model0TotalCorrect = OHETrainData.map(lambda point:  1 if model0.predict(point.features) == point.label else 0).sum()\n",
    "print model0TotalCorrect\n",
    "OHETrainDataCount = OHETrainData.count()\n",
    "print OHETrainDataCount\n",
    "print \"Accuracy on training set:\" \n",
    "print float(model0TotalCorrect) / float(OHETrainDataCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> LATER, ADD VALIDATION STEP:  ITERATE OVER HYPERPARAMETERS TO FIND BEST COMBINATION."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "print model0TotalCorrect\n",
    "OHETrainDataCount = OHETrainData.count()\n",
    "print OHETrainDataCount\n",
    "print \"Accuracy on training set:\" \n",
    "print float(model0TotalCorrect) / float(OHETrainDataCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "OHETrainData.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature vector:  commentLength, posNegDiff, timeSince, C[LoL,pics,politics] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "print sortedWeights, model0.intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 9.0 Evaluate results using test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Calculate accuracy = number of correctly classified examples / total number of examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "for key in srDigestR.keys():\n",
    "    print key, df2.filter(df2['subreddit'] == key).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "model0Predictions = OHETrainData.map(lambda point: model0.predict(point.features))\n",
    "print model0Predictions.take(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "numUp = model0Predictions.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "print numUp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "OHETrainDataLabels = OHETrainData.map(lambda point: point.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "numUpActual = OHETrainDataLabels.sum()\n",
    "print numUpActual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> TO DO:  Calculate ROC AUC (receiver operating characteristic - area under curve).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>TO DO:  calculate confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "trainingPredictionsAuto=(OHETrainData\n",
    "                     .map(lambda lpoint: model0.predict(lpoint.features))\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "print trainingPredictionsAuto.take(150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "numNotNinesAuto = trainingPredictionsAuto.filter(lambda P: 1-P > 0.001).count() # how many predictions are not ~= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "print numNotNinesAuto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "print trainingPredictionsAuto.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model with default values.  Any better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "modelDefault = LogisticRegressionWithSGD.train(OHETrainData, iterations = numIters,\n",
    "                                        intercept = includeIntercept)\n",
    "sortedWeights = sorted(modelDefault.weights)\n",
    "# print sortedWeights[:5], model0.intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "print sortedWeights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "print modelDefault.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "trainingPredDefault=(OHETrainData\n",
    "                     .map(lambda lpoint: modelDefault.predict(lpoint.features))\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "print trainingPredDefault.take(150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "print trainingPredDefault.filter(lambda P: 1-P > 0.001).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def bucketFeatByCount(featCount):\n",
    "    \"\"\"Bucket the counts by powers of two.\"\"\"\n",
    "    for i in range(11):\n",
    "        size = 2 ** i\n",
    "        if featCount <= size:\n",
    "            return size\n",
    "    return -1\n",
    "\n",
    "featCounts = (OHETrainData\n",
    "              .flatMap(lambda lp: lp.features.indices)\n",
    "              .map(lambda x: (x, 1))\n",
    "              .reduceByKey(lambda x, y: x + y))\n",
    "featCountsBuckets = (featCounts\n",
    "                     .map(lambda x: (bucketFeatByCount(x[1]), 1))\n",
    "                     .filter(lambda (k, v): k != -1)\n",
    "                     .reduceByKey(lambda x, y: x + y)\n",
    "                     .collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "x, y = zip(*featCountsBuckets)\n",
    "x, y = np.log(x), np.log(y)\n",
    "\n",
    "def preparePlot(xticks, yticks, figsize=(10.5, 6), hideLabels=False, gridColor='#999999',\n",
    "                gridWidth=1.0):\n",
    "    \"\"\"Template for generating the plot layout.\"\"\"\n",
    "    plt.close()\n",
    "    fig, ax = plt.subplots(figsize=figsize, facecolor='white', edgecolor='white')\n",
    "    ax.axes.tick_params(labelcolor='#999999', labelsize='10')\n",
    "    for axis, ticks in [(ax.get_xaxis(), xticks), (ax.get_yaxis(), yticks)]:\n",
    "        axis.set_ticks_position('none')\n",
    "        axis.set_ticks(ticks)\n",
    "        axis.label.set_color('#999999')\n",
    "        if hideLabels: axis.set_ticklabels([])\n",
    "    plt.grid(color=gridColor, linewidth=gridWidth, linestyle='-')\n",
    "    map(lambda position: ax.spines[position].set_visible(False), ['bottom', 'top', 'left', 'right'])\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "labelsAndScores = OHEValData.map(lambda lp:\n",
    "                                            (lp.label, getP(lp.features, model0.weights, model0.intercept)))\n",
    "labelsAndWeights = labelsAndScores.collect()\n",
    "labelsAndWeights.sort(key=lambda (k, v): v, reverse=True)\n",
    "labelsByWeight = np.array([k for (k, v) in labelsAndWeights])\n",
    "\n",
    "length = labelsByWeight.size\n",
    "truePositives = labelsByWeight.cumsum()\n",
    "numPositive = truePositives[-1]\n",
    "falsePositives = np.arange(1.0, length + 1, 1.) - truePositives\n",
    "\n",
    "truePositiveRate = truePositives / numPositive\n",
    "falsePositiveRate = falsePositives / (length - numPositive)\n",
    "\n",
    "# Generate layout and plot data\n",
    "fig, ax = preparePlot(np.arange(0., 1.1, 0.1), np.arange(0., 1.1, 0.1))\n",
    "ax.set_xlim(-.05, 1.05), ax.set_ylim(-.05, 1.05)\n",
    "ax.set_ylabel('True Positive Rate (Sensitivity)')\n",
    "ax.set_xlabel('False Positive Rate (1 - Specificity)')\n",
    "plt.plot(falsePositiveRate, truePositiveRate, color='#8cbfd0', linestyle='-', linewidth=3.)\n",
    "plt.plot((0., 1.), (0., 1.), linestyle='--', color='#d6ebf2', linewidth=2.)  # Baseline model\n",
    "pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
